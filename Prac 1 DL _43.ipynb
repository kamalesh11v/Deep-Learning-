{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7068c7e8",
   "metadata": {},
   "source": [
    "## Name : Kamleshwar Viyanwar \n",
    "## Batch : C3\n",
    "## Roll No : 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dd3ad88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ydata-profiling in c:\\users\\cse\\anaconda3\\lib\\site-packages (4.12.1)\n",
      "Requirement already satisfied: scipy<1.14,>=1.4.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.11.1)\n",
      "Requirement already satisfied: pandas!=1.4.0,<3,>1.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (2.0.3)\n",
      "Requirement already satisfied: matplotlib<3.10,>=3.5 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (3.7.2)\n",
      "Requirement already satisfied: pydantic>=2 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (2.10.3)\n",
      "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (6.0)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (3.1.2)\n",
      "Requirement already satisfied: visions[type_image_path]<0.7.7,>=0.7.5 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.7.6)\n",
      "Requirement already satisfied: numpy<2.2,>=1.16.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.24.3)\n",
      "Requirement already satisfied: htmlmin==0.1.12 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.1.12)\n",
      "Requirement already satisfied: phik<0.13,>=0.11.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.12.4)\n",
      "Requirement already satisfied: requests<3,>=2.24.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.48.2 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (4.65.0)\n",
      "Requirement already satisfied: seaborn<0.14,>=0.10.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.12.2)\n",
      "Requirement already satisfied: multimethod<2,>=1.4 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.12)\n",
      "Requirement already satisfied: statsmodels<1,>=0.13.2 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.14.0)\n",
      "Requirement already satisfied: typeguard<5,>=3 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (4.4.1)\n",
      "Requirement already satisfied: imagehash==4.3.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (4.3.1)\n",
      "Requirement already satisfied: wordcloud>=1.9.3 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.9.4)\n",
      "Requirement already satisfied: dacite>=1.8 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (1.8.1)\n",
      "Requirement already satisfied: numba<1,>=0.56.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from ydata-profiling) (0.57.1)\n",
      "Requirement already satisfied: PyWavelets in c:\\users\\cse\\anaconda3\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling) (1.4.1)\n",
      "Requirement already satisfied: pillow in c:\\users\\cse\\anaconda3\\lib\\site-packages (from imagehash==4.3.1->ydata-profiling) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from jinja2<3.2,>=2.11.1->ydata-profiling) (2.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib<3.10,>=3.5->ydata-profiling) (2.8.2)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from numba<1,>=0.56.0->ydata-profiling) (0.40.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from pandas!=1.4.0,<3,>1.1->ydata-profiling) (2023.3)\n",
      "Requirement already satisfied: joblib>=0.14.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from phik<0.13,>=0.11.1->ydata-profiling) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from pydantic>=2->ydata-profiling) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from pydantic>=2->ydata-profiling) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from pydantic>=2->ydata-profiling) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from requests<3,>=2.24.0->ydata-profiling) (2023.7.22)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from statsmodels<1,>=0.13.2->ydata-profiling) (0.5.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tqdm<5,>=4.48.2->ydata-profiling) (0.4.6)\n",
      "Requirement already satisfied: attrs>=19.3.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling) (22.1.0)\n",
      "Requirement already satisfied: networkx>=2.4 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from visions[type_image_path]<0.7.7,>=0.7.5->ydata-profiling) (3.1)\n",
      "Requirement already satisfied: six in c:\\users\\cse\\anaconda3\\lib\\site-packages (from patsy>=0.5.2->statsmodels<1,>=0.13.2->ydata-profiling) (1.16.0)\n",
      "Collecting livelossplot\n",
      "  Obtaining dependency information for livelossplot from https://files.pythonhosted.org/packages/e3/fd/07864f0fdb2d279d2b777fd6ac1ce6e31d19b23b7bc9807145b076d49f2b/livelossplot-0.5.5-py3-none-any.whl.metadata\n",
      "  Using cached livelossplot-0.5.5-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\cse\\anaconda3\\lib\\site-packages (from livelossplot) (3.7.2)\n",
      "Requirement already satisfied: bokeh in c:\\users\\cse\\anaconda3\\lib\\site-packages (from livelossplot) (3.2.1)\n",
      "Requirement already satisfied: Jinja2>=2.9 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (1.24.3)\n",
      "Requirement already satisfied: packaging>=16.8 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (23.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (2.0.3)\n",
      "Requirement already satisfied: pillow>=7.1.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (9.4.0)\n",
      "Requirement already satisfied: PyYAML>=3.10 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (6.0)\n",
      "Requirement already satisfied: tornado>=5.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (6.3.2)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from bokeh->livelossplot) (2022.9.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib->livelossplot) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib->livelossplot) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib->livelossplot) (1.4.4)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib->livelossplot) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from matplotlib->livelossplot) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from pandas>=1.2->bokeh->livelossplot) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from pandas>=1.2->bokeh->livelossplot) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n",
      "Using cached livelossplot-0.5.5-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: livelossplot\n",
      "Successfully installed livelossplot-0.5.5\n",
      "Requirement already satisfied: tensorflow in c:\\users\\cse\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.5.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\cse\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.9.1)\n",
      "Requirement already satisfied: namex in c:\\users\\cse\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\cse\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\cse\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ydata-profiling\n",
    "!pip install livelossplot\n",
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7a941f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>total_charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.9240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>18.0</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.5522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>28.0</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.4620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>33.0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.4700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>32.0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.8552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_id   age     sex     bmi  children smoker     region  total_charges\n",
       "0          1  19.0  female  27.900         0    yes  southwest     16884.9240\n",
       "1          2  18.0    male  33.770         1     no  southeast      1725.5522\n",
       "2          3  28.0    male  33.000         3     no  southeast      4449.4620\n",
       "3          4  33.0    male  22.705         0     no  northwest     21984.4700\n",
       "4          5  32.0    male  28.880         0     no  northwest      3866.8552"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('insurance.csv')\n",
    "\n",
    "# Check the first few rows\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d337db32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cse\\anaconda3\\Lib\\site-packages\\ydata_profiling\\profile_report.py:358: UserWarning: Try running command: 'pip install --upgrade Pillow' to avoid ValueError\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1e8c743e9c45a98ff6a736be024bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63be5356e654a768e9844dc7b79c940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddbe98cdcb241058765127aa8880184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render HTML:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65692e4269ac4a218435a557073ebc24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Export report to file:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "# Generate EDA report\n",
    "profile = ProfileReport(data, title=\"Insurance EDA Report\", explorative=True)\n",
    "profile.to_file(\"insurance_eda_report.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19c44c",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9de4415c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in dataset: Index(['person_id', 'age', 'sex', 'bmi', 'children', 'smoker', 'region',\n",
      "       'total_charges'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cse\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 289626688.0000 - mae: 12610.7295 - val_loss: 309690688.0000 - val_mae: 12388.3760\n",
      "Epoch 2/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 281184384.0000 - mae: 12022.5459 - val_loss: 262746016.0000 - val_mae: 10585.9482\n",
      "Epoch 3/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 205826656.0000 - mae: 9662.1191 - val_loss: 210516336.0000 - val_mae: 10209.5576\n",
      "Epoch 4/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 202319712.0000 - mae: 10225.2246 - val_loss: 213282032.0000 - val_mae: 10638.4658\n",
      "Epoch 5/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 186682944.0000 - mae: 9892.4619 - val_loss: 211160176.0000 - val_mae: 10402.0469\n",
      "Epoch 6/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 190442192.0000 - mae: 9867.0244 - val_loss: 211774352.0000 - val_mae: 10492.1553\n",
      "Epoch 7/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 180240448.0000 - mae: 9556.9395 - val_loss: 212106944.0000 - val_mae: 10540.0752\n",
      "Epoch 8/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 196701968.0000 - mae: 10013.8672 - val_loss: 211020608.0000 - val_mae: 10416.9824\n",
      "Epoch 9/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 176330336.0000 - mae: 9756.5498 - val_loss: 210753232.0000 - val_mae: 10391.0537\n",
      "Epoch 10/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 182514720.0000 - mae: 9681.8662 - val_loss: 211097312.0000 - val_mae: 10451.1758\n",
      "Epoch 11/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 190879248.0000 - mae: 9851.4873 - val_loss: 210996272.0000 - val_mae: 10454.4971\n",
      "Epoch 12/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 183130224.0000 - mae: 9833.0537 - val_loss: 211485168.0000 - val_mae: 10528.9824\n",
      "Epoch 13/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 188985952.0000 - mae: 9978.8057 - val_loss: 210040160.0000 - val_mae: 10365.7002\n",
      "Epoch 14/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 185742656.0000 - mae: 9668.8838 - val_loss: 211855760.0000 - val_mae: 10606.4131\n",
      "Epoch 15/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 182258336.0000 - mae: 9923.4531 - val_loss: 211738368.0000 - val_mae: 10619.6182\n",
      "Epoch 16/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 176840960.0000 - mae: 9589.1602 - val_loss: 210203840.0000 - val_mae: 10485.4053\n",
      "Epoch 17/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 183901120.0000 - mae: 9890.1494 - val_loss: 209157424.0000 - val_mae: 10394.6074\n",
      "Epoch 18/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 179052576.0000 - mae: 9584.5596 - val_loss: 210110880.0000 - val_mae: 10550.2891\n",
      "Epoch 19/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 182967232.0000 - mae: 9606.9688 - val_loss: 207190992.0000 - val_mae: 10158.0537\n",
      "Epoch 20/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 184911280.0000 - mae: 9654.7773 - val_loss: 208793504.0000 - val_mae: 10518.3711\n",
      "Epoch 21/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 187455936.0000 - mae: 9870.2031 - val_loss: 206069984.0000 - val_mae: 10266.8857\n",
      "Epoch 22/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 178545856.0000 - mae: 9520.5020 - val_loss: 207753920.0000 - val_mae: 10560.1855\n",
      "Epoch 23/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 190889776.0000 - mae: 9891.2188 - val_loss: 204934864.0000 - val_mae: 10375.0820\n",
      "Epoch 24/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 174426768.0000 - mae: 9656.3994 - val_loss: 203572496.0000 - val_mae: 10356.6709\n",
      "Epoch 25/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 194085440.0000 - mae: 10234.6787 - val_loss: 200895856.0000 - val_mae: 10216.8184\n",
      "Epoch 26/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 175577664.0000 - mae: 9646.6328 - val_loss: 198269632.0000 - val_mae: 10099.5479\n",
      "Epoch 27/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 184813424.0000 - mae: 9888.9756 - val_loss: 198587248.0000 - val_mae: 10323.7949\n",
      "Epoch 28/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 166080848.0000 - mae: 9141.8389 - val_loss: 196165424.0000 - val_mae: 10303.4355\n",
      "Epoch 29/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 162911216.0000 - mae: 9234.6523 - val_loss: 189643056.0000 - val_mae: 9903.4980\n",
      "Epoch 30/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 174636656.0000 - mae: 9520.7100 - val_loss: 185137936.0000 - val_mae: 9699.3857\n",
      "Epoch 31/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 171756288.0000 - mae: 9493.4658 - val_loss: 180317760.0000 - val_mae: 9559.8057\n",
      "Epoch 32/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 155783584.0000 - mae: 8809.5791 - val_loss: 175897504.0000 - val_mae: 9629.6973\n",
      "Epoch 33/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 158990384.0000 - mae: 9217.4316 - val_loss: 171511840.0000 - val_mae: 9040.7656\n",
      "Epoch 34/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 133832800.0000 - mae: 8035.7773 - val_loss: 165466016.0000 - val_mae: 9421.8701\n",
      "Epoch 35/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 134113112.0000 - mae: 8258.4473 - val_loss: 156843696.0000 - val_mae: 9004.2080\n",
      "Epoch 36/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 131733816.0000 - mae: 8079.9727 - val_loss: 149548000.0000 - val_mae: 8911.6191\n",
      "Epoch 37/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 130716720.0000 - mae: 8048.4600 - val_loss: 143389232.0000 - val_mae: 8441.9326\n",
      "Epoch 38/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 126292136.0000 - mae: 8115.4790 - val_loss: 140774608.0000 - val_mae: 8087.5532\n",
      "Epoch 39/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 133215880.0000 - mae: 8089.7349 - val_loss: 138980896.0000 - val_mae: 7968.3369\n",
      "Epoch 40/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 113967392.0000 - mae: 7346.3584 - val_loss: 125729632.0000 - val_mae: 8720.0859\n",
      "Epoch 41/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 105194472.0000 - mae: 7717.2124 - val_loss: 129068976.0000 - val_mae: 9140.0107\n",
      "Epoch 42/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 109781512.0000 - mae: 7916.4136 - val_loss: 108762448.0000 - val_mae: 7787.3057\n",
      "Epoch 43/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 106407112.0000 - mae: 7647.5298 - val_loss: 113994664.0000 - val_mae: 8528.5947\n",
      "Epoch 44/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 100630960.0000 - mae: 7695.2842 - val_loss: 100791896.0000 - val_mae: 7635.8555\n",
      "Epoch 45/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 97436584.0000 - mae: 7506.9990 - val_loss: 99848088.0000 - val_mae: 7045.9229\n",
      "Epoch 46/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 93166224.0000 - mae: 7131.2720 - val_loss: 93986896.0000 - val_mae: 7402.2441\n",
      "Epoch 47/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 96373016.0000 - mae: 7443.0010 - val_loss: 111650536.0000 - val_mae: 8736.6562\n",
      "Epoch 48/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 95661744.0000 - mae: 7582.7832 - val_loss: 88386896.0000 - val_mae: 7005.7935\n",
      "Epoch 49/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85670408.0000 - mae: 6837.3081 - val_loss: 86540624.0000 - val_mae: 7078.6387\n",
      "Epoch 50/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85346080.0000 - mae: 7118.3140 - val_loss: 87714480.0000 - val_mae: 7384.4531\n",
      "Epoch 51/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 85063880.0000 - mae: 7034.4067 - val_loss: 81938144.0000 - val_mae: 6873.8984\n",
      "Epoch 52/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84012944.0000 - mae: 6770.0044 - val_loss: 79093608.0000 - val_mae: 6585.7266\n",
      "Epoch 53/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78884560.0000 - mae: 6560.7271 - val_loss: 77703768.0000 - val_mae: 6671.7231\n",
      "Epoch 54/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82417904.0000 - mae: 6861.5630 - val_loss: 81563568.0000 - val_mae: 7270.3135\n",
      "Epoch 55/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 84907952.0000 - mae: 7138.1064 - val_loss: 74981112.0000 - val_mae: 6175.2256\n",
      "Epoch 56/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 82448640.0000 - mae: 6862.5190 - val_loss: 76398128.0000 - val_mae: 6958.9961\n",
      "Epoch 57/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 76598688.0000 - mae: 6653.9082 - val_loss: 71446016.0000 - val_mae: 6077.6968\n",
      "Epoch 58/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 73950928.0000 - mae: 6213.2871 - val_loss: 72030584.0000 - val_mae: 6701.0781\n",
      "Epoch 59/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 76126304.0000 - mae: 6675.2900 - val_loss: 70837336.0000 - val_mae: 6683.3154\n",
      "Epoch 60/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74914712.0000 - mae: 6487.7080 - val_loss: 66544624.0000 - val_mae: 6199.3413\n",
      "Epoch 61/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70114760.0000 - mae: 6429.6499 - val_loss: 72324400.0000 - val_mae: 6931.5166\n",
      "Epoch 62/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 66034540.0000 - mae: 6052.5571 - val_loss: 66434056.0000 - val_mae: 6459.2886\n",
      "Epoch 63/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 70254728.0000 - mae: 6449.3838 - val_loss: 63150140.0000 - val_mae: 6177.2764\n",
      "Epoch 64/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67864528.0000 - mae: 6083.3330 - val_loss: 61108284.0000 - val_mae: 5703.6250\n",
      "Epoch 65/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 65143148.0000 - mae: 6077.2183 - val_loss: 66331948.0000 - val_mae: 6723.2114\n",
      "Epoch 66/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 64833456.0000 - mae: 6175.8564 - val_loss: 77494920.0000 - val_mae: 7563.5928\n",
      "Epoch 67/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 68463872.0000 - mae: 6421.1738 - val_loss: 59959292.0000 - val_mae: 6199.9219\n",
      "Epoch 68/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56800220.0000 - mae: 5592.2134 - val_loss: 57978708.0000 - val_mae: 5350.4009\n",
      "Epoch 69/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57085000.0000 - mae: 5439.6938 - val_loss: 54980864.0000 - val_mae: 5391.7729\n",
      "Epoch 70/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 54308024.0000 - mae: 5504.6431 - val_loss: 57605796.0000 - val_mae: 6177.1055\n",
      "Epoch 71/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57054956.0000 - mae: 5813.0059 - val_loss: 52565120.0000 - val_mae: 5454.0806\n",
      "Epoch 72/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55467572.0000 - mae: 5541.8838 - val_loss: 53556636.0000 - val_mae: 5855.7388\n",
      "Epoch 73/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53587508.0000 - mae: 5621.2617 - val_loss: 50990892.0000 - val_mae: 5535.6211\n",
      "Epoch 74/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 49881752.0000 - mae: 5302.4800 - val_loss: 53228636.0000 - val_mae: 5044.2656\n",
      "Epoch 75/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 52885188.0000 - mae: 5327.6470 - val_loss: 50944360.0000 - val_mae: 5769.5312\n",
      "Epoch 76/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 52581924.0000 - mae: 5594.2744 - val_loss: 54415332.0000 - val_mae: 6236.4209\n",
      "Epoch 77/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 57686404.0000 - mae: 5827.0728 - val_loss: 47036816.0000 - val_mae: 4968.3701\n",
      "Epoch 78/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 53654156.0000 - mae: 5295.3901 - val_loss: 47852872.0000 - val_mae: 5557.8516\n",
      "Epoch 79/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48060364.0000 - mae: 5269.3696 - val_loss: 46341252.0000 - val_mae: 4817.9839\n",
      "Epoch 80/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 54814616.0000 - mae: 5385.1724 - val_loss: 48709076.0000 - val_mae: 5790.9116\n",
      "Epoch 81/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50281400.0000 - mae: 5178.1816 - val_loss: 44409172.0000 - val_mae: 5216.1846\n",
      "Epoch 82/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 43957512.0000 - mae: 5027.2954 - val_loss: 43944968.0000 - val_mae: 4699.0752\n",
      "Epoch 83/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 49673176.0000 - mae: 5214.3237 - val_loss: 54053080.0000 - val_mae: 4840.6167\n",
      "Epoch 84/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 52247312.0000 - mae: 5144.8032 - val_loss: 46178312.0000 - val_mae: 5634.0625\n",
      "Epoch 85/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 45327416.0000 - mae: 5107.4463 - val_loss: 42409724.0000 - val_mae: 5172.7861\n",
      "Epoch 86/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41407964.0000 - mae: 4770.0654 - val_loss: 41159212.0000 - val_mae: 4946.2666\n",
      "Epoch 87/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40489136.0000 - mae: 4881.0405 - val_loss: 41127132.0000 - val_mae: 5060.8398\n",
      "Epoch 88/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51088520.0000 - mae: 5347.9912 - val_loss: 49606572.0000 - val_mae: 6073.6455\n",
      "Epoch 89/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43122248.0000 - mae: 4898.3193 - val_loss: 39048764.0000 - val_mae: 4620.1836\n",
      "Epoch 90/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43252376.0000 - mae: 4720.4517 - val_loss: 50354204.0000 - val_mae: 6116.7754\n",
      "Epoch 91/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 47889912.0000 - mae: 5310.2358 - val_loss: 38280812.0000 - val_mae: 4665.8345\n",
      "Epoch 92/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 41671552.0000 - mae: 4834.2661 - val_loss: 44987068.0000 - val_mae: 5664.5742\n",
      "Epoch 93/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43882704.0000 - mae: 4831.3647 - val_loss: 39175048.0000 - val_mae: 4989.5356\n",
      "Epoch 94/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41065368.0000 - mae: 4736.6167 - val_loss: 36995156.0000 - val_mae: 4456.3740\n",
      "Epoch 95/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 44725248.0000 - mae: 4688.1211 - val_loss: 37888616.0000 - val_mae: 4290.7905\n",
      "Epoch 96/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46268920.0000 - mae: 4824.1499 - val_loss: 39936556.0000 - val_mae: 5126.4243\n",
      "Epoch 97/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38411436.0000 - mae: 4672.7695 - val_loss: 36105588.0000 - val_mae: 4328.4731\n",
      "Epoch 98/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 37554644.0000 - mae: 4423.7656 - val_loss: 36405528.0000 - val_mae: 4594.6646\n",
      "Epoch 99/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38088452.0000 - mae: 4462.1841 - val_loss: 38685476.0000 - val_mae: 5024.0513\n",
      "Epoch 100/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 39812564.0000 - mae: 4724.8320 - val_loss: 35636684.0000 - val_mae: 4527.7646\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32158026.0000 - mae: 4329.8574 \n",
      "Test Mean Absolute Error (MAE): 4527.76"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "data = pd.read_csv('insurance.csv')\n",
    "\n",
    "# Step 2: Check column names to ensure no discrepancies\n",
    "print(\"Columns in dataset:\", data.columns)\n",
    "\n",
    "# Step 3: Handle missing values\n",
    "# Impute missing values for numeric columns (mean imputation)\n",
    "numeric_columns = ['age', 'bmi', 'children']  # Specify numeric columns\n",
    "categorical_columns = ['sex', 'smoker', 'region']  # Specify categorical columns\n",
    "\n",
    "# Impute missing values for numeric columns with mean strategy\n",
    "imputer_numeric = SimpleImputer(strategy='mean')\n",
    "data[numeric_columns] = imputer_numeric.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Impute missing values for categorical columns with most frequent strategy\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "data[categorical_columns] = imputer_categorical.fit_transform(data[categorical_columns])\n",
    "\n",
    "# Step 4: Separate features (X) and target (y)\n",
    "X = data.drop('total_charges', axis=1)  # Features\n",
    "y = data['total_charges']  # Target variable\n",
    "\n",
    "# Step 5: Handle categorical data by one-hot encoding\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Step 6: Scale numeric features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X[['age', 'bmi', 'children']] = scaler.fit_transform(X[['age', 'bmi', 'children']])\n",
    "\n",
    "# Step 7: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 8: Build the Neural Network (DNN)\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),  # Input layer + hidden layer 1\n",
    "    Dense(64, activation='relu'),  # Hidden layer 2\n",
    "    Dense(32, activation='relu'),  # Hidden layer 3\n",
    "    Dense(1, activation='linear')  # Output layer (for regression)\n",
    "])\n",
    "\n",
    "# Step 9: Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Step 10: Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Step 11: Evaluate the model on the test set\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Mean Absolute Error (MAE): {test_mae:.2f}\")\n",
    "\n",
    "# Step 12: Save the trained model\n",
    "model.save(\"insurance_prediction_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc8b9184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "[[1.76343781e+02]\n",
      " [1.64161591e+02]\n",
      " [5.70764375e+04]\n",
      " [1.41528711e+03]\n",
      " [4.85087461e+04]\n",
      " [1.94112573e+03]\n",
      " [1.61866760e+02]\n",
      " [8.66875684e+03]\n",
      " [2.25850464e+02]\n",
      " [2.65538306e+03]\n",
      " [4.97543164e+04]\n",
      " [1.01502838e+03]\n",
      " [1.65335052e+02]\n",
      " [6.37204805e+04]\n",
      " [6.60425469e+04]\n",
      " [6.22129219e+04]\n",
      " [5.56438965e+03]\n",
      " [5.78510469e+04]\n",
      " [2.91540314e+02]\n",
      " [5.06060781e+04]\n",
      " [2.65539307e+02]\n",
      " [1.20274072e+03]\n",
      " [1.12709991e+02]\n",
      " [1.63752441e+02]\n",
      " [8.36497864e+02]\n",
      " [2.73482819e+02]\n",
      " [3.00364038e+03]\n",
      " [1.64032669e+02]\n",
      " [1.83840546e+02]\n",
      " [5.45809288e+01]\n",
      " [3.91926331e+02]\n",
      " [1.70783276e+03]\n",
      " [1.64074921e+02]\n",
      " [1.66825851e+02]\n",
      " [5.80542564e+01]\n",
      " [1.08037549e+03]\n",
      " [1.71597824e+02]\n",
      " [1.38609375e+02]\n",
      " [5.27173203e+04]\n",
      " [5.69601914e+04]\n",
      " [1.87900055e+02]\n",
      " [6.85329971e+01]\n",
      " [2.45114209e+03]\n",
      " [2.91418359e+03]\n",
      " [1.87339502e+03]\n",
      " [3.38481519e+03]\n",
      " [1.64894516e+02]\n",
      " [1.74369263e+02]\n",
      " [6.02586250e+04]\n",
      " [2.29678931e+03]\n",
      " [6.00563525e+03]\n",
      " [5.51500359e+01]\n",
      " [1.26342065e+03]\n",
      " [1.65222763e+02]\n",
      " [6.19949365e+03]\n",
      " [7.74397888e+02]\n",
      " [1.62362122e+02]\n",
      " [5.45519023e+04]\n",
      " [2.08378638e+03]\n",
      " [2.48734558e+02]\n",
      " [2.84504272e+03]\n",
      " [1.68891431e+03]\n",
      " [7.97306738e+03]\n",
      " [1.70374573e+02]\n",
      " [1.82915710e+02]\n",
      " [1.65214645e+02]\n",
      " [4.79882344e+04]\n",
      " [4.02662354e+03]\n",
      " [1.63878830e+02]\n",
      " [1.28310150e+02]\n",
      " [1.28010437e+02]\n",
      " [1.93073639e+02]\n",
      " [1.16077649e+03]\n",
      " [2.00027026e+03]\n",
      " [4.70143018e+03]\n",
      " [1.44393585e+02]\n",
      " [7.73336487e+01]\n",
      " [8.76716614e+02]\n",
      " [2.32152466e+02]\n",
      " [1.57224518e+02]\n",
      " [5.61048241e+01]\n",
      " [5.59427578e+04]\n",
      " [1.75376617e+02]\n",
      " [4.57589766e+04]\n",
      " [5.96983516e+04]\n",
      " [5.43559336e+04]\n",
      " [1.14610291e+03]\n",
      " [4.15703955e+03]\n",
      " [1.83750946e+02]\n",
      " [5.83828174e+03]\n",
      " [2.78635522e+03]\n",
      " [5.69946523e+04]\n",
      " [4.83211055e+04]\n",
      " [1.51122345e+02]\n",
      " [5.62723672e+04]\n",
      " [7.19139099e+02]\n",
      " [5.32380547e+04]\n",
      " [1.66140961e+02]\n",
      " [4.66804492e+04]\n",
      " [1.68072998e+02]\n",
      " [1.65390991e+02]\n",
      " [2.67912537e+02]\n",
      " [4.19951855e+03]\n",
      " [6.49632959e+03]\n",
      " [9.26391968e+02]\n",
      " [1.50328064e+02]\n",
      " [2.71776392e+03]\n",
      " [5.59085391e+04]\n",
      " [1.63877365e+02]\n",
      " [4.87980000e+04]\n",
      " [7.18274460e+01]\n",
      " [3.38201685e+03]\n",
      " [5.20441040e+02]\n",
      " [4.66963047e+04]\n",
      " [1.79863907e+02]\n",
      " [6.42209396e+01]\n",
      " [1.40352100e+03]\n",
      " [5.10626016e+04]\n",
      " [3.07738312e+02]\n",
      " [9.68062439e+01]\n",
      " [1.67801941e+02]\n",
      " [1.98458020e+03]\n",
      " [9.02076111e+02]\n",
      " [1.86552277e+02]\n",
      " [1.65279419e+02]\n",
      " [2.13346069e+02]\n",
      " [1.08486548e+03]\n",
      " [1.45883514e+02]\n",
      " [4.35221533e+03]\n",
      " [1.17884717e+03]\n",
      " [1.68031494e+02]\n",
      " [2.54351367e+03]\n",
      " [7.96687866e+02]\n",
      " [1.97200867e+02]\n",
      " [2.65427063e+02]\n",
      " [6.63076904e+03]\n",
      " [1.03968636e+02]\n",
      " [5.43198516e+04]\n",
      " [5.59112500e+04]\n",
      " [4.67010586e+04]\n",
      " [1.64633667e+02]\n",
      " [1.96587112e+02]\n",
      " [2.08975342e+02]\n",
      " [3.11673218e+03]\n",
      " [7.06666031e+01]\n",
      " [5.55703828e+04]\n",
      " [1.14746262e+02]\n",
      " [2.60009186e+02]\n",
      " [3.09750146e+03]\n",
      " [6.23434372e+01]\n",
      " [5.91863828e+04]\n",
      " [1.62967682e+02]\n",
      " [6.14059692e+02]\n",
      " [5.00677734e+04]\n",
      " [9.09874878e+02]\n",
      " [8.65078812e+01]\n",
      " [2.48461157e+03]\n",
      " [4.96804749e+02]\n",
      " [4.73439336e+04]\n",
      " [5.21474531e+04]\n",
      " [5.80908630e+02]\n",
      " [1.64413300e+02]\n",
      " [4.47339941e+03]\n",
      " [1.64600601e+02]\n",
      " [6.58770676e+01]\n",
      " [4.38671973e+03]\n",
      " [6.07597734e+04]\n",
      " [5.66720508e+04]\n",
      " [5.27787656e+04]\n",
      " [2.52568115e+02]\n",
      " [2.02013641e+02]\n",
      " [1.37699036e+02]\n",
      " [9.47295044e+02]\n",
      " [7.23307037e+01]\n",
      " [2.85029175e+02]\n",
      " [5.63523633e+04]\n",
      " [4.63419375e+04]\n",
      " [1.10667803e+04]\n",
      " [4.43360703e+04]\n",
      " [8.54030701e+02]\n",
      " [5.34396797e+04]\n",
      " [1.64129822e+02]\n",
      " [5.40051270e+02]\n",
      " [1.65883911e+02]\n",
      " [1.63853592e+02]\n",
      " [1.62245422e+02]\n",
      " [1.61027161e+02]\n",
      " [2.74008728e+02]\n",
      " [5.46757471e+03]\n",
      " [2.50151196e+03]\n",
      " [1.53257965e+02]\n",
      " [1.64908813e+02]\n",
      " [1.71326324e+02]\n",
      " [5.44329883e+04]\n",
      " [9.78275293e+03]\n",
      " [1.91123047e+03]\n",
      " [1.61747498e+02]\n",
      " [2.53896347e+02]\n",
      " [1.63647064e+02]\n",
      " [5.08821045e+02]\n",
      " [1.64108368e+02]\n",
      " [5.67339727e+04]\n",
      " [3.97771777e+03]\n",
      " [9.16001511e+01]\n",
      " [4.55026797e+04]\n",
      " [4.63145312e+04]\n",
      " [1.16492090e+03]\n",
      " [1.65456711e+02]\n",
      " [6.16855811e+03]\n",
      " [2.10207809e+02]\n",
      " [2.54640820e+03]\n",
      " [1.20665979e+03]\n",
      " [2.77881909e+03]\n",
      " [5.02066445e+04]\n",
      " [5.64942566e+02]\n",
      " [1.68648941e+02]\n",
      " [1.61584656e+02]\n",
      " [3.72759106e+03]\n",
      " [1.64356458e+03]\n",
      " [2.25635605e+02]\n",
      " [1.61468582e+02]\n",
      " [3.62190332e+03]\n",
      " [7.23100293e+03]\n",
      " [6.06550156e+04]\n",
      " [6.74578857e+01]\n",
      " [5.26923750e+04]\n",
      " [3.48932892e+02]\n",
      " [1.13926781e+02]\n",
      " [3.67942969e+03]\n",
      " [6.39839844e+03]\n",
      " [1.80270828e+02]\n",
      " [3.10591736e+02]\n",
      " [1.72829971e+02]\n",
      " [5.37401953e+04]\n",
      " [3.65886414e+02]\n",
      " [5.13957129e+03]\n",
      " [1.15040405e+02]\n",
      " [9.48896179e+02]\n",
      " [5.82900625e+04]\n",
      " [6.89798126e+01]\n",
      " [3.09811279e+02]\n",
      " [5.35056328e+04]\n",
      " [1.64276947e+02]\n",
      " [1.65234406e+02]\n",
      " [1.60016144e+02]\n",
      " [6.26978645e+01]\n",
      " [1.75028717e+02]\n",
      " [6.06461792e+01]\n",
      " [9.93072266e+03]\n",
      " [1.60639496e+02]\n",
      " [2.61235046e+02]\n",
      " [1.91395447e+03]\n",
      " [2.94073792e+02]\n",
      " [2.34064728e+02]\n",
      " [1.62676270e+02]\n",
      " [1.56918549e+02]\n",
      " [3.74900171e+03]\n",
      " [1.10877800e+02]\n",
      " [2.01727997e+02]\n",
      " [1.67642868e+02]\n",
      " [2.11326813e+02]\n",
      " [1.09093982e+03]\n",
      " [5.13552656e+04]\n",
      " [6.46153281e+04]\n",
      " [1.14199585e+03]\n",
      " [1.42885117e+02]\n",
      " [6.38485000e+04]\n",
      " [1.00932983e+03]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load the saved model\n",
    "loaded_model = load_model('insurance_prediction_model.h5')\n",
    "\n",
    "# Step 2: Load the data (or use existing data if you already have it)\n",
    "# Assuming you already have the X_test data in the correct format\n",
    "# Make sure to apply the same preprocessing you did when training the model (e.g., scaling)\n",
    "\n",
    "# Let's assume X_test is the test data you want to predict on.\n",
    "# If you haven't standardized your test data yet, you need to do it:\n",
    "scaler = StandardScaler()\n",
    "X_test_std = scaler.fit_transform(X_test)  # Assuming X_test is your test features\n",
    "\n",
    "# Step 3: Make predictions\n",
    "predictions = loaded_model.predict(X_test_std)\n",
    "\n",
    "# Step 4: Check the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5615a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled with loss and metrics: mean_squared_error ['loss', 'compile_metrics']\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cse\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/34\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 1s/step - loss: 321991456.0000 - mae: 12402.7500"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 63\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeys in history:\u001b[39m\u001b[38;5;124m\"\u001b[39m, history\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Assuming X_train, y_train, X_test, and y_test are preprocessed and ready\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m train_and_plot(X_train, y_train, X_test, y_test)\n",
      "Cell \u001b[1;32mIn[16], line 56\u001b[0m, in \u001b[0;36mtrain_and_plot\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     53\u001b[0m live_plot \u001b[38;5;241m=\u001b[39m LiveLossPlot()\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Train the model with the custom callback\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_test, y_test), callbacks\u001b[38;5;241m=\u001b[39m[live_plot])\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Check if 'loss' and 'val_loss' are available in the history\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeys in history:\u001b[39m\u001b[38;5;124m\"\u001b[39m, history\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mkeys())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[16], line 29\u001b[0m, in \u001b[0;36mLiveLossPlot.on_epoch_end\u001b[1;34m(self, epoch, logs)\u001b[0m\n\u001b[0;32m     26\u001b[0m plt\u001b[38;5;241m.\u001b[39mclf()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Plot the loss and validation loss\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'loss'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "158c579b",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "211cef72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cse\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 12/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 13/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 14/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 15/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 16/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 17/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 18/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 19/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 20/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 21/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 22/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 23/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 24/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 25/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 26/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 27/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 28/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 29/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 30/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 31/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 32/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 33/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 34/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 35/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 36/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 37/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 38/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 39/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 40/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 41/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 42/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 43/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 44/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 45/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 46/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 47/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 48/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 49/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 50/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 51/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 52/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 53/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 54/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 55/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 56/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 57/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 58/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 59/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 60/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 61/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 62/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 63/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 64/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 65/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 66/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 67/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 68/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 69/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 70/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 71/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 72/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 73/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 74/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 75/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 76/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 77/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 78/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 79/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 80/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 81/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 82/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 83/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 84/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 85/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 86/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 87/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 88/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 89/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 90/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 91/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 92/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 93/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 94/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 95/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 96/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 97/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 98/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 99/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 100/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron model saved as 'insurance_perceptron_model.h5'\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Define the Perceptron model (Single-layer)\n",
    "model_perceptron = Sequential([\n",
    "    Dense(1, input_dim=X_train.shape[1], activation='linear')  # Output layer for regression\n",
    "])\n",
    "\n",
    "# Compile the model with Stochastic Gradient Descent (SGD) optimizer\n",
    "model_perceptron.compile(optimizer=SGD(), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history_perceptron = model_perceptron.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_perceptron.save('insurance_perceptron_model.h5')\n",
    "print(\"Perceptron model saved as 'insurance_perceptron_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03d14ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "Predictions on the test set:\n",
      "[[nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]\n",
      " [nan]]\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "loaded_perceptron_model = load_model('insurance_perceptron_model.h5')\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions_perceptron = loaded_perceptron_model.predict(X_test)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predictions on the test set:\")\n",
    "print(predictions_perceptron)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c912911d",
   "metadata": {},
   "source": [
    "## Define DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DNN model\n",
    "model_dnn = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_dnn.compile(optimizer=SGD(), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history_dnn = model_dnn.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d53fda",
   "metadata": {},
   "source": [
    "## Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "517b2b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cse\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 317723840.0000 - mae: 13192.8447 - val_loss: 287856288.0000 - val_mae: 11443.7822\n",
      "Epoch 2/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 266994880.0000 - mae: 11216.7295 - val_loss: 213680848.0000 - val_mae: 10670.1436\n",
      "Epoch 3/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 195637104.0000 - mae: 10177.1680 - val_loss: 212970960.0000 - val_mae: 10616.1738\n",
      "Epoch 4/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 184154144.0000 - mae: 9710.1191 - val_loss: 213537616.0000 - val_mae: 10680.7402\n",
      "Epoch 5/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 196600240.0000 - mae: 10217.9990 - val_loss: 211365056.0000 - val_mae: 10466.3379\n",
      "Epoch 6/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 201194880.0000 - mae: 10013.9824 - val_loss: 211529504.0000 - val_mae: 10502.0996\n",
      "Epoch 7/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 176882208.0000 - mae: 9661.9668 - val_loss: 211659792.0000 - val_mae: 10532.9082\n",
      "Epoch 8/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 193972336.0000 - mae: 9979.0645 - val_loss: 212047696.0000 - val_mae: 10592.9453\n",
      "Epoch 9/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 172576240.0000 - mae: 9404.8193 - val_loss: 212684704.0000 - val_mae: 10670.5156\n",
      "Epoch 10/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 184391552.0000 - mae: 9834.1436 - val_loss: 209972000.0000 - val_mae: 10403.3789\n",
      "Epoch 11/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 188125296.0000 - mae: 9973.0820 - val_loss: 209445152.0000 - val_mae: 10390.7979\n",
      "Epoch 12/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 185554288.0000 - mae: 9902.9932 - val_loss: 212590560.0000 - val_mae: 10742.2188\n",
      "Epoch 13/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 188351376.0000 - mae: 10031.6074 - val_loss: 207650784.0000 - val_mae: 10279.2393\n",
      "Epoch 14/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 183989936.0000 - mae: 9646.8848 - val_loss: 206431840.0000 - val_mae: 10208.9043\n",
      "Epoch 15/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 179982608.0000 - mae: 9614.0908 - val_loss: 206444032.0000 - val_mae: 10355.0469\n",
      "Epoch 16/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 196604624.0000 - mae: 10122.7275 - val_loss: 208733344.0000 - val_mae: 10671.3711\n",
      "Epoch 17/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 189483296.0000 - mae: 9935.6104 - val_loss: 203727920.0000 - val_mae: 10326.8262\n",
      "Epoch 18/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 174133056.0000 - mae: 9597.9736 - val_loss: 200007120.0000 - val_mae: 10053.4502\n",
      "Epoch 19/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 179832240.0000 - mae: 9511.0273 - val_loss: 197359152.0000 - val_mae: 10041.0713\n",
      "Epoch 20/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 193423536.0000 - mae: 9906.7559 - val_loss: 194048976.0000 - val_mae: 9984.5889\n",
      "Epoch 21/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 174120128.0000 - mae: 9441.5703 - val_loss: 188674992.0000 - val_mae: 9866.4434\n",
      "Epoch 22/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 162306976.0000 - mae: 9227.8496 - val_loss: 183392384.0000 - val_mae: 9856.8242\n",
      "Epoch 23/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 158108416.0000 - mae: 8894.3740 - val_loss: 177282512.0000 - val_mae: 9260.0439\n",
      "Epoch 24/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 150104960.0000 - mae: 8791.7559 - val_loss: 174890560.0000 - val_mae: 9039.3857\n",
      "Epoch 25/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 143983296.0000 - mae: 8532.4023 - val_loss: 155787024.0000 - val_mae: 9109.5391\n",
      "Epoch 26/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 134628416.0000 - mae: 8424.8682 - val_loss: 161429424.0000 - val_mae: 10015.0029\n",
      "Epoch 27/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 130287616.0000 - mae: 8347.7637 - val_loss: 142255728.0000 - val_mae: 9323.7871\n",
      "Epoch 28/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 118877224.0000 - mae: 7991.9243 - val_loss: 129327560.0000 - val_mae: 8907.0840\n",
      "Epoch 29/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 94986136.0000 - mae: 7047.3916 - val_loss: 115410472.0000 - val_mae: 8366.2285\n",
      "Epoch 30/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 103616432.0000 - mae: 7516.7632 - val_loss: 102882584.0000 - val_mae: 7627.4116\n",
      "Epoch 31/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 94545656.0000 - mae: 7199.8408 - val_loss: 95730648.0000 - val_mae: 7375.7656\n",
      "Epoch 32/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 89549304.0000 - mae: 7146.4766 - val_loss: 99085832.0000 - val_mae: 8005.9170\n",
      "Epoch 33/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 85260600.0000 - mae: 7100.0366 - val_loss: 85185800.0000 - val_mae: 6894.2017\n",
      "Epoch 34/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80220792.0000 - mae: 6463.2808 - val_loss: 83015704.0000 - val_mae: 6499.4482\n",
      "Epoch 35/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 91499160.0000 - mae: 6963.6826 - val_loss: 89003480.0000 - val_mae: 7679.7637\n",
      "Epoch 36/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78899992.0000 - mae: 6800.4316 - val_loss: 75568648.0000 - val_mae: 6527.4189\n",
      "Epoch 37/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 82246656.0000 - mae: 6753.9575 - val_loss: 74420976.0000 - val_mae: 6753.4873\n",
      "Epoch 38/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75572808.0000 - mae: 6530.5684 - val_loss: 73056920.0000 - val_mae: 6765.2549\n",
      "Epoch 39/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 82448800.0000 - mae: 6812.4492 - val_loss: 72892048.0000 - val_mae: 5876.5244\n",
      "Epoch 40/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 62026272.0000 - mae: 5835.1958 - val_loss: 76506432.0000 - val_mae: 7213.7114\n",
      "Epoch 41/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68332640.0000 - mae: 6132.1582 - val_loss: 93709536.0000 - val_mae: 8344.2930\n",
      "Epoch 42/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73949920.0000 - mae: 6459.3608 - val_loss: 61912212.0000 - val_mae: 6032.0679\n",
      "Epoch 43/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 64030448.0000 - mae: 6076.2217 - val_loss: 80797424.0000 - val_mae: 7691.8691\n",
      "Epoch 44/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 66279380.0000 - mae: 6275.8574 - val_loss: 59643228.0000 - val_mae: 6121.5571\n",
      "Epoch 45/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 63199672.0000 - mae: 5940.3926 - val_loss: 58803360.0000 - val_mae: 6159.3335\n",
      "Epoch 46/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 62728840.0000 - mae: 5974.7852 - val_loss: 54655460.0000 - val_mae: 5729.8213\n",
      "Epoch 47/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 64929268.0000 - mae: 5964.6147 - val_loss: 68951816.0000 - val_mae: 7167.0684\n",
      "Epoch 48/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 56732556.0000 - mae: 5827.1392 - val_loss: 53936920.0000 - val_mae: 5933.5024\n",
      "Epoch 49/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 55488032.0000 - mae: 5520.4976 - val_loss: 56066092.0000 - val_mae: 6280.5420\n",
      "Epoch 50/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 53214964.0000 - mae: 5630.3184 - val_loss: 48443928.0000 - val_mae: 5026.4033\n",
      "Epoch 51/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 57004584.0000 - mae: 5219.7188 - val_loss: 49697160.0000 - val_mae: 4919.5049\n",
      "Epoch 52/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50661972.0000 - mae: 5324.5269 - val_loss: 48538424.0000 - val_mae: 5709.0703\n",
      "Epoch 53/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46227296.0000 - mae: 5148.6646 - val_loss: 44297260.0000 - val_mae: 5103.8716\n",
      "Epoch 54/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 50002632.0000 - mae: 5333.4854 - val_loss: 43672864.0000 - val_mae: 4748.6885\n",
      "Epoch 55/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42435140.0000 - mae: 4803.2266 - val_loss: 42134064.0000 - val_mae: 4750.5908\n",
      "Epoch 56/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42579944.0000 - mae: 4980.4492 - val_loss: 43595404.0000 - val_mae: 5365.0557\n",
      "Epoch 57/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 51277888.0000 - mae: 5261.9512 - val_loss: 42396568.0000 - val_mae: 4544.0039\n",
      "Epoch 58/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 44171800.0000 - mae: 5030.7812 - val_loss: 40153728.0000 - val_mae: 4883.4492\n",
      "Epoch 59/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 48497952.0000 - mae: 5074.0361 - val_loss: 39252012.0000 - val_mae: 4605.9995\n",
      "Epoch 60/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41937624.0000 - mae: 4923.4048 - val_loss: 38727968.0000 - val_mae: 4544.2227\n",
      "Epoch 61/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42663168.0000 - mae: 4764.6528 - val_loss: 38034852.0000 - val_mae: 4430.0703\n",
      "Epoch 62/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 46327612.0000 - mae: 5023.6709 - val_loss: 38899460.0000 - val_mae: 4923.7524\n",
      "Epoch 63/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43141652.0000 - mae: 4898.2026 - val_loss: 39687040.0000 - val_mae: 5088.1069\n",
      "Epoch 64/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39444216.0000 - mae: 4756.6255 - val_loss: 36863600.0000 - val_mae: 4592.4697\n",
      "Epoch 65/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38128792.0000 - mae: 4616.2754 - val_loss: 37631696.0000 - val_mae: 4807.1509\n",
      "Epoch 66/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40727856.0000 - mae: 4679.2568 - val_loss: 43308012.0000 - val_mae: 5433.8101\n",
      "Epoch 67/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40166824.0000 - mae: 4684.6411 - val_loss: 35448648.0000 - val_mae: 4386.8555\n",
      "Epoch 68/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39322292.0000 - mae: 4509.9756 - val_loss: 37020740.0000 - val_mae: 4716.4478\n",
      "Epoch 69/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41141596.0000 - mae: 4674.5127 - val_loss: 39654048.0000 - val_mae: 5091.4966\n",
      "Epoch 70/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39020640.0000 - mae: 4609.9648 - val_loss: 39279376.0000 - val_mae: 5061.3184\n",
      "Epoch 71/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37709984.0000 - mae: 4485.9351 - val_loss: 34618492.0000 - val_mae: 4160.4209\n",
      "Epoch 72/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42927752.0000 - mae: 4645.8789 - val_loss: 37358808.0000 - val_mae: 4868.6631\n",
      "Epoch 73/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43617012.0000 - mae: 4699.5156 - val_loss: 34568020.0000 - val_mae: 4340.5894\n",
      "Epoch 74/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40091508.0000 - mae: 4509.5620 - val_loss: 34566000.0000 - val_mae: 4400.0342\n",
      "Epoch 75/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38098844.0000 - mae: 4399.1406 - val_loss: 34888592.0000 - val_mae: 4079.3176\n",
      "Epoch 76/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 43093972.0000 - mae: 4603.6436 - val_loss: 33811416.0000 - val_mae: 4215.4282\n",
      "Epoch 77/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35259916.0000 - mae: 4198.9697 - val_loss: 36951128.0000 - val_mae: 4768.2935\n",
      "Epoch 78/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37697856.0000 - mae: 4248.9243 - val_loss: 37562316.0000 - val_mae: 4896.0659\n",
      "Epoch 79/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42603596.0000 - mae: 4588.2827 - val_loss: 35375424.0000 - val_mae: 4615.2686\n",
      "Epoch 80/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35957160.0000 - mae: 4230.6670 - val_loss: 35106012.0000 - val_mae: 3973.5820\n",
      "Epoch 81/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38920904.0000 - mae: 4378.6851 - val_loss: 33762356.0000 - val_mae: 4287.3599\n",
      "Epoch 82/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 37374764.0000 - mae: 4278.5586 - val_loss: 39841392.0000 - val_mae: 4062.0708\n",
      "Epoch 83/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 45844712.0000 - mae: 4683.5225 - val_loss: 38638936.0000 - val_mae: 4958.9429\n",
      "Epoch 84/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40360064.0000 - mae: 4648.1592 - val_loss: 34418712.0000 - val_mae: 4461.4854\n",
      "Epoch 85/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 36663524.0000 - mae: 4182.4424 - val_loss: 33342002.0000 - val_mae: 3937.5127\n",
      "Epoch 86/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36619720.0000 - mae: 4200.5083 - val_loss: 32648620.0000 - val_mae: 3962.1511\n",
      "Epoch 87/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39433392.0000 - mae: 4403.9233 - val_loss: 40431400.0000 - val_mae: 5058.0298\n",
      "Epoch 88/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37973636.0000 - mae: 4389.0908 - val_loss: 32878630.0000 - val_mae: 4007.9001\n",
      "Epoch 89/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38799336.0000 - mae: 4315.2954 - val_loss: 33852252.0000 - val_mae: 4402.7837\n",
      "Epoch 90/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35778124.0000 - mae: 4173.2656 - val_loss: 32353032.0000 - val_mae: 3930.1899\n",
      "Epoch 91/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39460340.0000 - mae: 4320.5674 - val_loss: 32640848.0000 - val_mae: 4154.2778\n",
      "Epoch 92/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36865556.0000 - mae: 4235.6318 - val_loss: 33650440.0000 - val_mae: 4374.6670\n",
      "Epoch 93/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39616252.0000 - mae: 4300.9209 - val_loss: 33185892.0000 - val_mae: 4288.8770\n",
      "Epoch 94/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39069876.0000 - mae: 4356.2700 - val_loss: 41926404.0000 - val_mae: 5143.4414\n",
      "Epoch 95/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35054748.0000 - mae: 4280.0850 - val_loss: 49926412.0000 - val_mae: 5651.0103\n",
      "Epoch 96/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36531828.0000 - mae: 4155.4136 - val_loss: 36993920.0000 - val_mae: 3961.4395\n",
      "Epoch 97/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 40837204.0000 - mae: 4415.9209 - val_loss: 33005832.0000 - val_mae: 4255.3140\n",
      "Epoch 98/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37736860.0000 - mae: 4114.6802 - val_loss: 32637432.0000 - val_mae: 3882.6323\n",
      "Epoch 99/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 44199464.0000 - mae: 4336.2759 - val_loss: 32253876.0000 - val_mae: 4112.9795\n",
      "Epoch 100/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39990200.0000 - mae: 4350.1289 - val_loss: 34361300.0000 - val_mae: 4426.4834\n",
      "Epoch 101/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39625192.0000 - mae: 4411.0615 - val_loss: 32446216.0000 - val_mae: 4113.7925\n",
      "Epoch 102/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36984668.0000 - mae: 4247.1006 - val_loss: 38161880.0000 - val_mae: 4931.1753\n",
      "Epoch 103/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34251980.0000 - mae: 4146.8945 - val_loss: 33836332.0000 - val_mae: 4477.4150\n",
      "Epoch 104/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32499020.0000 - mae: 3871.1052 - val_loss: 33028096.0000 - val_mae: 4227.6675\n",
      "Epoch 105/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 39778716.0000 - mae: 4301.2866 - val_loss: 33721216.0000 - val_mae: 3820.5852\n",
      "Epoch 106/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39587348.0000 - mae: 4291.4536 - val_loss: 36301728.0000 - val_mae: 4692.0840\n",
      "Epoch 107/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38693012.0000 - mae: 4361.3291 - val_loss: 31528378.0000 - val_mae: 3748.8892\n",
      "Epoch 108/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33641540.0000 - mae: 4063.2676 - val_loss: 36967156.0000 - val_mae: 4723.9487\n",
      "Epoch 109/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39635692.0000 - mae: 4421.6421 - val_loss: 33215354.0000 - val_mae: 4406.7632\n",
      "Epoch 110/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36607164.0000 - mae: 4165.9673 - val_loss: 33358298.0000 - val_mae: 4302.4199\n",
      "Epoch 111/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38194040.0000 - mae: 4192.6479 - val_loss: 37764048.0000 - val_mae: 4843.0884\n",
      "Epoch 112/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35990680.0000 - mae: 4188.6719 - val_loss: 31185188.0000 - val_mae: 3988.1973\n",
      "Epoch 113/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42133192.0000 - mae: 4459.4136 - val_loss: 32043584.0000 - val_mae: 3718.6821\n",
      "Epoch 114/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35695384.0000 - mae: 3950.3557 - val_loss: 35437520.0000 - val_mae: 4642.0439\n",
      "Epoch 115/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35254072.0000 - mae: 4027.0237 - val_loss: 32679440.0000 - val_mae: 4271.2739\n",
      "Epoch 116/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32025476.0000 - mae: 3864.5349 - val_loss: 30500354.0000 - val_mae: 3797.5586\n",
      "Epoch 117/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38947212.0000 - mae: 4095.6504 - val_loss: 31145310.0000 - val_mae: 3670.8059\n",
      "Epoch 118/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 42947756.0000 - mae: 4465.0366 - val_loss: 30775688.0000 - val_mae: 3717.5884\n",
      "Epoch 119/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34438048.0000 - mae: 3955.1201 - val_loss: 30374976.0000 - val_mae: 3802.8208\n",
      "Epoch 120/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35873856.0000 - mae: 4066.7283 - val_loss: 31801234.0000 - val_mae: 3682.8918\n",
      "Epoch 121/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31975034.0000 - mae: 3826.6636 - val_loss: 31234288.0000 - val_mae: 3980.8577\n",
      "Epoch 122/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37048396.0000 - mae: 4123.5659 - val_loss: 31144948.0000 - val_mae: 4058.2468\n",
      "Epoch 123/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31639332.0000 - mae: 3715.5195 - val_loss: 30411280.0000 - val_mae: 3700.6143\n",
      "Epoch 124/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32643274.0000 - mae: 3829.6255 - val_loss: 31004930.0000 - val_mae: 4093.5205\n",
      "Epoch 125/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39092544.0000 - mae: 4184.5322 - val_loss: 31438208.0000 - val_mae: 4054.5503\n",
      "Epoch 126/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 41092540.0000 - mae: 4330.8628 - val_loss: 36936460.0000 - val_mae: 4923.8594\n",
      "Epoch 127/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 41984224.0000 - mae: 4374.8579 - val_loss: 31790830.0000 - val_mae: 4172.2856\n",
      "Epoch 128/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36547132.0000 - mae: 4196.5703 - val_loss: 30244438.0000 - val_mae: 3641.4326\n",
      "Epoch 129/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39826512.0000 - mae: 4257.4854 - val_loss: 30602798.0000 - val_mae: 3742.0449\n",
      "Epoch 130/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33234180.0000 - mae: 3899.6843 - val_loss: 30282348.0000 - val_mae: 3613.4089\n",
      "Epoch 131/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34180640.0000 - mae: 3937.6599 - val_loss: 37426680.0000 - val_mae: 4790.8242\n",
      "Epoch 132/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 38121352.0000 - mae: 4216.0718 - val_loss: 38105496.0000 - val_mae: 4689.2134\n",
      "Epoch 133/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31773638.0000 - mae: 3866.6968 - val_loss: 36459856.0000 - val_mae: 4420.6353\n",
      "Epoch 134/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37760744.0000 - mae: 4119.0137 - val_loss: 29960072.0000 - val_mae: 3867.1973\n",
      "Epoch 135/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32269144.0000 - mae: 3765.9592 - val_loss: 29239114.0000 - val_mae: 3666.5823\n",
      "Epoch 136/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 31779400.0000 - mae: 3712.7434 - val_loss: 29430972.0000 - val_mae: 3853.3218\n",
      "Epoch 137/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35687316.0000 - mae: 4046.8884 - val_loss: 30614686.0000 - val_mae: 3463.9282\n",
      "Epoch 138/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31940022.0000 - mae: 3734.5857 - val_loss: 28598560.0000 - val_mae: 3544.3538\n",
      "Epoch 139/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31959270.0000 - mae: 3646.8298 - val_loss: 31795812.0000 - val_mae: 3562.3284\n",
      "Epoch 140/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35957364.0000 - mae: 4073.7300 - val_loss: 39046204.0000 - val_mae: 4005.1040\n",
      "Epoch 141/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39223508.0000 - mae: 4249.6011 - val_loss: 50534388.0000 - val_mae: 5722.0962\n",
      "Epoch 142/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34558936.0000 - mae: 4048.0654 - val_loss: 30217502.0000 - val_mae: 3597.3796\n",
      "Epoch 143/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30236106.0000 - mae: 3705.8967 - val_loss: 28661082.0000 - val_mae: 3603.3291\n",
      "Epoch 144/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 34226432.0000 - mae: 3931.6282 - val_loss: 28626450.0000 - val_mae: 3594.5186\n",
      "Epoch 145/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32267596.0000 - mae: 3639.8184 - val_loss: 29223192.0000 - val_mae: 3866.9265\n",
      "Epoch 146/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32914702.0000 - mae: 3720.6221 - val_loss: 28295406.0000 - val_mae: 3471.7190\n",
      "Epoch 147/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32507564.0000 - mae: 3719.3491 - val_loss: 29660144.0000 - val_mae: 3772.5369\n",
      "Epoch 148/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 34840864.0000 - mae: 3866.4309 - val_loss: 34911752.0000 - val_mae: 4607.3696\n",
      "Epoch 149/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33906964.0000 - mae: 3918.8152 - val_loss: 33061128.0000 - val_mae: 4311.3555\n",
      "Epoch 150/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34824712.0000 - mae: 3963.4417 - val_loss: 28549462.0000 - val_mae: 3758.3721\n",
      "Epoch 151/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35236556.0000 - mae: 3898.9060 - val_loss: 28079094.0000 - val_mae: 3345.3550\n",
      "Epoch 152/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34722024.0000 - mae: 3759.1533 - val_loss: 30076466.0000 - val_mae: 3882.3555\n",
      "Epoch 153/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33555756.0000 - mae: 3740.3882 - val_loss: 27968338.0000 - val_mae: 3603.2542\n",
      "Epoch 154/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37121688.0000 - mae: 4084.6230 - val_loss: 39961740.0000 - val_mae: 4815.2593\n",
      "Epoch 155/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 39699908.0000 - mae: 4147.1724 - val_loss: 28522294.0000 - val_mae: 3688.8977\n",
      "Epoch 156/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35479068.0000 - mae: 3794.1509 - val_loss: 27618154.0000 - val_mae: 3488.3840\n",
      "Epoch 157/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35217560.0000 - mae: 3904.6550 - val_loss: 31241982.0000 - val_mae: 4109.6821\n",
      "Epoch 158/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32092112.0000 - mae: 3741.6084 - val_loss: 27476022.0000 - val_mae: 3380.2170\n",
      "Epoch 159/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29261532.0000 - mae: 3533.2896 - val_loss: 28275714.0000 - val_mae: 3762.8481\n",
      "Epoch 160/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34744768.0000 - mae: 3823.6492 - val_loss: 27689294.0000 - val_mae: 3338.4788\n",
      "Epoch 161/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30815180.0000 - mae: 3645.4072 - val_loss: 28890194.0000 - val_mae: 3281.0588\n",
      "Epoch 162/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34540824.0000 - mae: 3851.2083 - val_loss: 28503080.0000 - val_mae: 3836.8005\n",
      "Epoch 163/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30314118.0000 - mae: 3636.1057 - val_loss: 29312174.0000 - val_mae: 3310.8513\n",
      "Epoch 164/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29978062.0000 - mae: 3539.6821 - val_loss: 29979124.0000 - val_mae: 4132.7192\n",
      "Epoch 165/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 33445310.0000 - mae: 3788.9446 - val_loss: 26906974.0000 - val_mae: 3354.4683\n",
      "Epoch 166/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34419556.0000 - mae: 3719.0217 - val_loss: 26828360.0000 - val_mae: 3400.7834\n",
      "Epoch 167/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37174628.0000 - mae: 3985.3740 - val_loss: 30512334.0000 - val_mae: 3849.9131\n",
      "Epoch 168/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30468116.0000 - mae: 3561.3196 - val_loss: 34136008.0000 - val_mae: 3784.0454\n",
      "Epoch 169/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35695868.0000 - mae: 3903.9089 - val_loss: 36659500.0000 - val_mae: 4562.2144\n",
      "Epoch 170/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 37685708.0000 - mae: 4030.7615 - val_loss: 28595934.0000 - val_mae: 3511.2827\n",
      "Epoch 171/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 36380220.0000 - mae: 3852.2144 - val_loss: 32234672.0000 - val_mae: 4030.2070\n",
      "Epoch 172/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32338914.0000 - mae: 3649.2234 - val_loss: 32900164.0000 - val_mae: 4396.3813\n",
      "Epoch 173/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30782080.0000 - mae: 3606.9683 - val_loss: 26563074.0000 - val_mae: 3159.7947\n",
      "Epoch 174/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31519592.0000 - mae: 3501.0903 - val_loss: 26358446.0000 - val_mae: 3271.1816\n",
      "Epoch 175/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 30191470.0000 - mae: 3411.4038 - val_loss: 26495798.0000 - val_mae: 3464.1252\n",
      "Epoch 176/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29592440.0000 - mae: 3497.9424 - val_loss: 26659618.0000 - val_mae: 3264.2891\n",
      "Epoch 177/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35536608.0000 - mae: 3729.6060 - val_loss: 34913468.0000 - val_mae: 4444.3545\n",
      "Epoch 178/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29719956.0000 - mae: 3505.7373 - val_loss: 26310574.0000 - val_mae: 3235.4109\n",
      "Epoch 179/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31197266.0000 - mae: 3522.8130 - val_loss: 33859944.0000 - val_mae: 4213.6812\n",
      "Epoch 180/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 31972600.0000 - mae: 3434.5547 - val_loss: 29158192.0000 - val_mae: 3855.0449\n",
      "Epoch 181/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25231556.0000 - mae: 3152.5352 - val_loss: 33525332.0000 - val_mae: 4256.6865\n",
      "Epoch 182/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29241782.0000 - mae: 3543.3359 - val_loss: 28776924.0000 - val_mae: 3722.2974\n",
      "Epoch 183/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28992812.0000 - mae: 3353.1748 - val_loss: 26792218.0000 - val_mae: 3042.5237\n",
      "Epoch 184/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29547408.0000 - mae: 3468.4814 - val_loss: 37295316.0000 - val_mae: 4738.3369\n",
      "Epoch 185/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33483378.0000 - mae: 3712.8196 - val_loss: 30591608.0000 - val_mae: 3267.1858\n",
      "Epoch 186/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 25193456.0000 - mae: 3175.8274 - val_loss: 26471940.0000 - val_mae: 3419.0244\n",
      "Epoch 187/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 35457480.0000 - mae: 3796.4839 - val_loss: 39744868.0000 - val_mae: 4963.5269\n",
      "Epoch 188/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 32620146.0000 - mae: 3688.3521 - val_loss: 25964392.0000 - val_mae: 3140.6292\n",
      "Epoch 189/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 27794826.0000 - mae: 3284.6619 - val_loss: 27228274.0000 - val_mae: 3055.3418\n",
      "Epoch 190/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28120696.0000 - mae: 3244.3816 - val_loss: 25440762.0000 - val_mae: 3073.9790\n",
      "Epoch 191/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29615506.0000 - mae: 3488.3948 - val_loss: 26019698.0000 - val_mae: 3501.9285\n",
      "Epoch 192/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26368256.0000 - mae: 3245.8181 - val_loss: 25669886.0000 - val_mae: 3156.1929\n",
      "Epoch 193/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 34060576.0000 - mae: 3735.4678 - val_loss: 26690042.0000 - val_mae: 3037.3291\n",
      "Epoch 194/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26124050.0000 - mae: 3049.5408 - val_loss: 26017598.0000 - val_mae: 3426.9319\n",
      "Epoch 195/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 33796272.0000 - mae: 3565.6597 - val_loss: 25250804.0000 - val_mae: 3332.5339\n",
      "Epoch 196/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29480776.0000 - mae: 3446.7900 - val_loss: 30906482.0000 - val_mae: 4137.4351\n",
      "Epoch 197/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29713158.0000 - mae: 3486.1389 - val_loss: 25607826.0000 - val_mae: 3232.4377\n",
      "Epoch 198/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 28612982.0000 - mae: 3360.5337 - val_loss: 24328406.0000 - val_mae: 3097.9932\n",
      "Epoch 199/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 30134116.0000 - mae: 3337.4929 - val_loss: 26591172.0000 - val_mae: 3576.7786\n",
      "Epoch 200/200\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 29576896.0000 - mae: 3386.6379 - val_loss: 25295746.0000 - val_mae: 3440.0547\n"
     ]
    }
   ],
   "source": [
    "# Trying different learning rates\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Example of Adam optimizer with learning rate\n",
    "model_dnn_adam = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "\n",
    "# Compile the model with Adam optimizer and a learning rate of 0.001\n",
    "model_dnn_adam.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model with different epochs and batch sizes\n",
    "history_dnn_adam = model_dnn_adam.fit(X_train, y_train, epochs=200, batch_size=16, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08ed985",
   "metadata": {},
   "source": [
    "## SGD optimzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9097a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cse\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 12/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 13/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 14/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 15/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 16/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 17/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 18/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 19/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 20/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 21/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 22/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 23/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 24/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 25/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 26/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 27/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 28/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 29/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 30/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 31/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 32/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 33/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 34/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 35/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 36/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 37/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 38/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 39/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 40/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 41/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 42/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 43/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 44/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 45/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 46/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 47/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 48/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 49/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 50/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 51/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 52/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 53/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 54/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 55/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 56/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 57/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 58/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 59/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 60/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 61/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 62/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 63/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 64/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 65/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 66/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 67/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 68/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 69/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 70/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 71/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 72/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 73/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 74/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 75/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 76/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 77/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 78/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 79/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 80/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 81/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 82/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 83/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 84/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 85/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 86/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 87/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 88/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 89/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 90/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 91/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 92/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 93/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 94/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 95/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 96/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 97/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 98/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 99/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 100/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cse\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 12/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 13/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 14/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 15/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 16/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 17/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 18/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 19/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 20/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 21/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 22/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 23/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 24/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 25/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 26/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 27/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 28/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 29/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 30/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 31/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 32/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 33/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 34/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 35/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 36/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 37/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 38/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 39/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 40/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 41/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 42/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 43/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 44/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 45/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 46/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 47/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 48/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 49/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 50/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 51/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 52/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 53/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 54/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 55/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 56/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 57/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 58/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 59/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 60/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 61/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 62/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 63/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 64/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 65/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 66/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 67/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 68/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 69/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 70/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 71/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 72/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 73/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 74/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 75/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 76/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 77/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 78/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 79/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 80/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 81/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 82/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 83/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 84/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 85/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 86/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 87/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 88/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 89/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 90/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 91/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 92/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 93/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 94/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 95/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 96/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 97/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 98/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 99/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 100/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cse\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 2/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 3/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 4/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 5/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 6/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 7/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 8/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 9/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 10/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 11/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 12/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 13/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 14/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 15/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 16/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 17/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 18/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 19/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 20/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 21/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 22/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 23/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 24/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 25/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 26/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 27/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 28/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 29/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 30/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 31/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 32/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 33/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 34/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 35/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 36/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 37/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 38/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 39/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 40/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 41/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 42/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 43/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 44/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 45/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 46/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 47/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 48/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 49/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 50/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 51/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 52/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 53/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 54/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 55/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 56/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 57/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 58/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 59/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 60/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 61/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 62/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 63/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 64/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 65/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 66/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 67/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 68/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 69/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 70/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 71/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 72/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 73/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 74/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 75/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 76/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 77/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 78/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 79/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 80/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 81/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 82/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 83/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 84/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 85/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 86/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 87/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 88/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 89/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 90/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 91/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 92/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 93/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 94/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 95/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 96/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 97/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 98/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 99/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n",
      "Epoch 100/100\n",
      "\u001b[1m34/34\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: nan - mae: nan - val_loss: nan - val_mae: nan\n"
     ]
    }
   ],
   "source": [
    "# SGD Optimizer\n",
    "sgd_optimizer = SGD(learning_rate=0.01)\n",
    "model_dnn_sgd = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model_dnn_sgd.compile(optimizer=sgd_optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history_dnn_sgd = model_dnn_sgd.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Momentum Optimizer\n",
    "momentum_optimizer = SGD(learning_rate=0.01, momentum=0.9)\n",
    "model_dnn_momentum = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model_dnn_momentum.compile(optimizer=momentum_optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history_dnn_momentum = model_dnn_momentum.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Nesterov Accelerated Gradient\n",
    "nesterov_optimizer = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "model_dnn_nesterov = Sequential([\n",
    "    Dense(64, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model_dnn_nesterov.compile(optimizer=nesterov_optimizer, loss='mean_squared_error', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history_dnn_nesterov = model_dnn_nesterov.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17bb603f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "data = data.dropna()\n",
    "\n",
    "# Re-split features (X) and target (y) after dropping missing rows\n",
    "X = data.drop('total_charges', axis=1)\n",
    "y = data['total_charges']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f0fdb",
   "metadata": {},
   "source": [
    "## Comparing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f6516b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABv4AAAJNCAYAAAD54gLgAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAACZQ0lEQVR4nOzdeVwVZf//8feJ3e2kIBwxVCpDS0u/WorWDabibmXlQqKluYRp5FJxW6ZmmtRtfotMMxXXtEXbIzVNM5fcqFyyRe7UAjHDgxaBwvz+8Md8PbJ4UBEZX8/HYx51rvlcc10z5wzn8nxmrrEZhmEIAAAAAAAAAAAAQIV2VXl3AAAAAAAAAAAAAMCFI/EHAAAAAAAAAAAAWACJPwAAAAAAAAAAAMACSPwBAAAAAAAAAAAAFkDiDwAAAAAAAAAAALAAEn8AAAAAAAAAAACABZD4AwAAAAAAAAAAACyAxB8AAAAAAAAAAABgAST+AAAAAAAAAAAAAAsg8QeUQlJSkmw2m2w2m7788stC6w3D0PXXXy+bzabIyMhL3r/SOHnypGbNmqVbb71VNWrUUKVKlVS3bl3dddddWrFiRXl3r8x9+eWXxb6PZzrzPS9qOVf98jZgwAB17NjRfP3f//7X7Pv48eOLrVMQcyZ3PzNntlHUcma7MTExuvvuuy/mLgMALMJK464z/c///I9sNpteeumlItePHz9eNptNf/zxxyXtV7169fTggw+eV90lS5Zo+vTpF7U/0v8di+KW//73vxe9zYupbdu2Gjp0qPm6YPxps9mUlJRUZJ0777xTNptN9erVK3L9yZMn5XA4ZLPZ9O677xYZ4+5xy8zM1NVXX63333//AvYSAIArz9m/FXl6eqpWrVrq3bu3fvrpJ5fYvLw8TZs2TR07dtQ111yjSpUqqWHDhnrqqad07Ngxt9r766+/NHXqVN1yyy2qVq2aqlatquuuu049e/bUunXrymAPAVR0nuXdAaAiqlq1qubMmVPoR6Z169bpl19+UdWqVcunY6UQExOj5cuXKy4uThMmTJCPj4/279+v5ORkff7557rnnnvKu4uXlXnz5qlBgwaFym+88cZy6I17du7cqfnz52vLli2F1lWtWlVJSUkaN26crrrq/64BOXHihN555x1Vq1ZNWVlZLnVK+5kZPny4oqOjC7V9zTXXmP8/fvx4NWjQQGvWrNGdd955obsMALAgK4y7CqSkpGjnzp2SpDlz5mj06NHl3KOLY8mSJdq1a5fi4uLKZPvJycmy2+2FymvVqlUm7V0MH3zwgb7++mstWLCg0LqCz/TZidbU1FR9+eWXqlatWrHb/fjjj3X48GFJpz9D9913X7Gx5zpu1atX1+OPP64xY8aoc+fO8vb2dmfXAADA/1fwW9E///yjr7/+Ws8//7zWrl2rH374QdWrV5ckZWdna/z48erTp48efvhhBQQEaMeOHZo0aZI++ugjbdu2TX5+fsW2kZeXp6ioKH3//fcaM2aMbrvtNknSTz/9pI8++khfffWVIiIiLsn+Aqg4SPwB56FXr15avHixXnvtNZd/mM+ZM0fh4eGFEiaXm9TUVC1btkzjxo3ThAkTzPK2bdtq0KBBys/PL8felcwwDP3zzz8lDorKQqNGjdS8efNS1Smpr9nZ2fL19S10V11p/P3336pUqVKx61944QXddtttRfa7V69eevPNN/XFF1+offv2ZvmyZcuUl5enu+++W4sWLTLLz+czU6dOHbVs2bLEfbjuuuvUsWNHvfDCCyT+AABFqujjrjO9+eabkqQuXbrok08+0caNG9WqVaty7tXlr1mzZgoICChVnby8PJ06dUo+Pj6F1p1rDOWO7OzsEsejkydP1j333KPatWsXWlcwDvvpp59Uv359s3zu3LmqXbu2GjdurD179hS53Tlz5sjb21sRERFauXKlDh065HJR1ZncOW5Dhw7VpEmT9O677xZ5wRYAACjemb8VRUZGKi8vT88++6zef/99PfTQQ5IkPz8/paamyt/f36wXGRmpOnXq6P7779d7772nvn37FtvG+vXrtXHjRs2dO9fcpiR16NBBjz766CX9Da+k8RWAywtTfQLnoU+fPpKkt956yyxzOp167733NGDAgCLr5ObmatKkSWrQoIF8fHxUs2ZNPfTQQzpy5IhL3LJlyxQVFaVatWrJz8/PvP3/r7/+col78MEHVaVKFf3888/q3LmzqlSpopCQEI0aNUo5OTkl9v/o0aOSir9K+sw7wCTphx9+UMeOHVWpUiUFBARo6NCh+uijjwpNvVXcFFGRkZEuV+n/888/GjVqlJo0aSK73a4aNWooPDxcH3zwQaG6NptNjz76qGbOnKmGDRvKx8dH8+fPl3T66qbo6GgFBgbKx8dHDRs21GuvvVZoG0X1//jx4yUeo/NRXF8LpoBYuXKlBgwYoJo1a6pSpUrKyclRfn6+EhISzM9FYGCg+vXrp0OHDrlsOzIyUo0aNdL69evVqlUrVapUqdjPmiQdPnxYK1asUExMTJHrw8LC1KpVK82dO9elfO7cuerRo0ehq8NL+5kpjZiYGK1evVq//PLLeW8DAGBdFX3cVeCff/7RkiVL1KxZM7388suSVOh7+EwHDx5Ujx49VK1aNdntdvXt27dQ/9esWaPIyEj5+/vLz89PderU0b333qu///7bjPnzzz8VGxur2rVry9vbW9dee63Gjh17zn4XjF/Onk7z7OnSIyMj9cknn+jXX391mfKqgLvvxYUomGY8ISFBkyZNUmhoqHx8fLR27Vpz2ssdO3bovvvuU/Xq1XXddddJOv2exMfHKzQ0VN7e3qpdu7aGDRtWaNqtevXqqWvXrlq+fLmaNm0qX19flwuhzrZz50598803xY7D2rdvr5CQEJf3Pz8/X/Pnz1f//v2LHVf9/vvvSk5OVrdu3TRmzBjl5+cXO2Wou4KCgtS+fXvNnDnzgrYDAABkJgEL7s6XJA8PD5ekX4GCO/cOHjxY4jZL+3vMb7/9psGDByskJETe3t4KDg7Wfffd59KnAwcOqG/fvi6/p/3nP/9xSSKWNL6SpG3btql79+6qUaOGfH191bRpU7399tsl7guAS4fEH3AeqlWrpvvuu8/lH+tvvfWWrrrqKvXq1atQfH5+vu666y698MILio6O1ieffKIXXnhBq1atUmRkpLKzs83Yn376SZ07d9acOXOUnJysuLg4vf322+rWrVuh7Z48eVLdu3dX27Zt9cEHH2jAgAF6+eWXNXXq1BL737BhQ1199dWaMGGC3njjjRKfj3L48GFFRERo165dmjFjhhYuXKgTJ07o0UcfdeNIFS0nJ0d//vmnRo8erffff19vvfWWbr/9dvXo0aPI6ZDef/99vf766xo3bpw+//xz3XHHHdqzZ49uvfVW7dq1S//5z3/08ccfq0uXLhoxYoTLDzEXq/8FVzWdueTl5bnV1wIDBgyQl5eXFi5cqHfffVdeXl565JFH9OSTT6p9+/b68MMP9dxzzyk5OVmtWrUq9GyftLQ09e3bV9HR0fr0008VGxtbbH9XrlypkydPqk2bNsXGDBw4UO+//74yMzMlSfv27dPGjRs1cODAQrGl+cwUyM/PL3TMTp06VSguMjJShmHo008/Pec2AQBXnoo+7iqwfPlyZWZmasCAAapfv75uv/12LVu2TCdOnCgy/p577tH111+vd999V+PHj9f777+vDh066OTJk5JO/xjTpUsXeXt7a+7cuUpOTtYLL7ygypUrKzc3V9LpxFabNm20YMECjRw5Up988on69u2rhIQE9ejRw61+n8uMGTPUunVrORwObdq0yVyk0r0XJXF3HPbKK69ozZo1eumll/TZZ5+5TNPeo0cPXX/99XrnnXc0c+ZMGYahu+++Wy+99JJiYmL0ySefaOTIkZo/f77uvPPOQonRHTt2aMyYMRoxYoSSk5N17733Ftvfjz/+WB4eHvrXv/5V5PqrrrpKDz74oBYsWGDuR8Hde2deyX+2pKQk5eXlacCAAWrXrp3q1q2ruXPnyjCMCzpukZGR+vrrr91+zhAAAChaamqqJOmGG244Z+yaNWskSTfddFOJcc2bN5eXl5cee+wxLV68WGlpacXG/vbbb7r11lu1YsUKjRw5Up999pmmT58uu91u/vZz5MgRtWrVSitXrtRzzz2nDz/8UO3atdPo0aOL/K2sqPHV2rVr1bp1ax07dkwzZ87UBx98oCZNmqhXr14XfFESgIvEAOC2efPmGZKMrVu3GmvXrjUkGbt27TIMwzBuvfVW48EHHzQMwzBuuukmIyIiwqz31ltvGZKM9957z2V7W7duNSQZM2bMKLK9/Px84+TJk8a6desMSca3335rruvfv78hyXj77bdd6nTu3NkICws757588sknRkBAgCHJkGT4+/sb999/v/Hhhx+6xD355JOGzWYzUlJSXMrbt29vSDLWrl1rltWtW9fo379/obYiIiJcjsfZTp06ZZw8edIYOHCg0bRpU5d1kgy73W78+eefLuUdOnQwrrnmGsPpdLqUP/roo4avr68ZX5r+F6XgPS9q8fDwcKuvBdvo16+fS/nevXsNSUZsbKxL+ZYtWwxJxr///W+zLCIiwpBkfPHFFyX2t8Ajjzxi+Pn5Gfn5+S7lqamphiTjxRdfNI4fP25UqVLFSExMNAzDMMaMGWOEhoYa+fn5xrBhw4yzvyLc/cwUtFHc8tVXXxXqb+3atY1evXq5tW8AgCuDlcZdhmEYd955p+Hr62tkZma67N+cOXNc4p599llDkvH444+7lC9evNiQZCxatMgwDMN49913DUmFxjhnmjlzZpH9njp1qiHJWLlypVl29jiuoH+pqakudQveizPHUF26dDHq1q1bqP3zfS8KFByLopbrrrvOjCsYe1x33XVGbm5ukdsYN26cS3lycrIhyUhISHApX7ZsmSHJeOONN8yyunXrGh4eHsa+fftK7G+BTp06GQ0aNChUXnDs3nnnHWP//v2GzWYzPv74Y8MwDOP+++83IiMjDcMo+njm5+cb119/vVG7dm3j1KlTLvt29vjQ3eNWYNWqVYYk47PPPnNr/wAAuNIVjJM2b95snDx50jh+/LiRnJxsOBwO41//+pdx8uTJEusfOnTICAoKMpo3b27k5eWds705c+YYVapUMb/Pa9WqZfTr189Yv369S9yAAQMMLy8vY8+ePcVu66mnnjIkGVu2bHEpf+SRRwybzWaOd0oaXzVo0MBo2rRpof3s2rWrUatWLbf2CUDZuqLv+Fu/fr26deum4OBg2Ww2vf/++6Xexueff66WLVuqatWqqlmzpu69917z6g5YW0REhK677jrNnTtX33//vbZu3VrsdFMff/yxrr76anXr1s3litsmTZrI4XC4TJe5f/9+RUdHy+FwyMPDQ15eXuZDevfu3euyXZvNVuiK9Jtvvlm//vrrOfvfuXNnHThwQCtWrNDo0aN100036f3331f37t1drvBZu3atbrrpJt1yyy0u9S/0GSDvvPOOWrdurSpVqsjT01NeXl6aM2dOoX2UpDvvvNN8KLJ0+ur1L774Qvfcc48qVarkckw7d+6sf/75R5s3b76o/V+wYIG2bt3qsmzZsuWcfT3T2VeGF0yPcPb0qLfddpsaNmyoL774wqW8evXqbj8H7/fff1fNmjVLfIZglSpVdP/992vu3Lk6deqUFixYoIceeqjYOu5+Zgo89thjhY7Z1q1b1aRJk0KxgYGB+u2339zaNwDAlaeij7tSU1O1du1a9ejRQ1dffbUk6f7771fVqlWLne7zgQcecHnds2dPeXp6muOHJk2ayNvbW4MHD9b8+fO1f//+QttYs2aNKleurPvuu8+lvGDscfZY42IrzXtRktWrVxcaTxT1b7fu3bvLy8uryG2cPQ4ruMr+7HHY/fffr8qVKxc6NjfffLNbV+9Lp8dhgYGBJcaEhoYqMjJSc+fO1dGjR827SIuzbt06/fzzz+rfv788PDwkyRy3FfcZcve4FfSVsRgAAKXTsmVLeXl5qWrVqurYsaOqV6+uDz74QJ6ensXW+fPPP9W5c2cZhqFly5a59eiUAQMG6NChQ1qyZIlGjBihkJAQLVq0SBEREXrxxRfNuM8++0xt2rRRw4YNi93WmjVrdOONN5pTjRZ48MEHZRiGOUYqcPb46ueff9YPP/xgjlXP/k0uLS1N+/btO+c+AShbxf8VugL89ddfuuWWW/TQQw+VOFVLcfbv36+77rpLI0eO1OLFi+V0OvX444+rR48e2rlzZxn0GJcTm82mhx56SK+88or++ecf3XDDDS7TOp7p8OHDOnbsmLy9vYtcXzCl44kTJ3THHXfI19dXkyZN0g033KBKlSqZz3g5ezqkSpUqydfX16XMx8dH//zzj1v74Ofnp7vvvlt33323pNNzfHfq1EmvvfaaHnnkEd100006evSoQkNDC9V1OBxutVGU5cuXq2fPnrr//vs1ZswYORwOeXp66vXXXy/yh4uz5zE/evSoTp06pVdffVWvvvpqkW0UHNOL1f+GDRuac7WXpLg514taV9I87cHBwYV+SCxp22fLzs4u9NkoysCBA3X77bfr+eef15EjR4p8RuOZ3PnMFLjmmmvcOmaS5Ovr6/Z0XwCAK09FH3cVTMd43333uUyn2L17dy1evFg//PCDy7SUUuGxiqenp/z9/c3xw3XXXafVq1crISFBw4YN019//aVrr71WI0aM0GOPPSbp9FjD4XAUuqgnMDBQnp6e5rbKirvvxbnccsstCggIOGdcacdhnp6eqlmzpku5zWaTw+EodGxKOw4LCgo6Z9zAgQP10EMPadq0afLz8yuUoD3TnDlzJJ2eArbgM2S323X77bfrvffeU2JioplULuDucSv4XDMWAwCgdBYsWKCGDRvq+PHjWrZsmWbNmqU+ffros88+KzI+MzNT7du312+//aY1a9bo2muvdbstu92uPn36mM+/3r17t9q1a6exY8dq0KBBuvrqq3XkyBFdc801JW7n6NGjqlevXqHy4OBgc/2Zzh4DFTwrcPTo0Ro9enSRbbg7xgNQdq7oxF+nTp3UqVOnYtfn5ubq6aef1uLFi3Xs2DE1atRIU6dOVWRkpKTTz3nIy8vTpEmTzKszRo8erbvuuksnT54s9mpTWMeDDz6ocePGaebMmXr++eeLjQsICJC/v7+Sk5OLXF+1alVJp6+6+f333/Xll1+aV5tLumTP26hTp44GDx6suLg47d69WzfddJP8/f2Vnp5eKLaoMl9f30LPQ5FOf+Gf+aPDokWLFBoaqmXLlrn8EFVUXUmFfqyqXr26PDw8FBMTo2HDhhVZpyDZV5r+Xwwl3WF39rqChzunpaUVGpj9/vvvhX6oKWnbZwsICNCOHTvOGde6dWuFhYVp4sSJat++vUJCQtxuQyr6M3M+/vzzzyIHngAAFKio4678/HzzWSfFPVdv7ty5SkhIcClLT09X7dq1zdenTp3S0aNHzfGDJN1xxx264447lJeXp23btunVV19VXFycgoKC1Lt3b/n7+2vLli0yDMNlHJGRkaFTp06VmBQqSAadPT4rzQ857r4XF0tpx2GnTp3SkSNHXJJ/hmEoPT1dt956q9vbPltAQID+/PPPc8b16NFDw4YN0wsvvKBBgwbJz8+vyDin06n33ntPkgr1q8CSJUtKfP5zSQr66k6SEAAA/J8zLxJv06aN8vLy9Oabb+rdd98tdEFPZmam2rVrp9TUVH3xxRe6+eabL6jtm266Sb1799b06dP1448/6rbbblPNmjV16NChEuv5+/sX+ZzA33//XVLh8cDZY6CC9fHx8cWObcPCwtzeDwBl44qe6vNcHnroIX399ddaunSpvvvuO91///3q2LGjfvrpJ0mnH67q4eGhefPmKS8vT06nUwsXLlRUVBRJvytE7dq1NWbMGHXr1k39+/cvNq5r1646evSo8vLy1Lx580JLwRdiwZepj4+PS/1Zs2Zd1H4fP35cJ06cKHJdwbRWBVf6tGnTRrt379a3337rErdkyZJCdevVq6fvvvvOpezHH38sdIu/zWaTt7e3y+AhPT1dH3zwgVv9r1Spktq0aaOdO3fq5ptvLvKYFvwoVpr+X2oF03YuWrTIpXzr1q3au3ev2rZte97bbtCggY4ePSqn03nO2KefflrdunXTqFGjio0pzWemtE6dOqWDBw/qxhtvPK/6AIArQ0Udd33++ec6dOiQhg0bprVr1xZabrrpJi1YsECnTp1yqbd48WKX12+//bZOnTplXoR4Jg8PD7Vo0UKvvfaaJJkX/7Rt21YnTpwoNL3jggULzPXFKbgg5+yx3Ycfflgo1sfHp8i7xdx9L8pDwb6fPQ5777339Ndff13wOKyoqVfP5ufnp3Hjxqlbt2565JFHio1bsmSJsrOz9dxzzxX5GQoICCh2uk93FPSVsRgAABcmISFB1atX17hx45Sfn2+WFyT99u/fr5UrV6pp06Zub/Po0aPKzc0tct0PP/wg6f9+j+nUqZPWrl1b4lSbbdu21Z49ewpdLL5gwQLZbDa1adOmxP6EhYWpfv36+vbbb4sc3zVv3vyiX9wFoPSu6Dv+SvLLL7/orbfe0qFDh8w/nqNHj1ZycrLmzZunyZMnq169elq5cqXuv/9+DRkyRHl5eQoPD9enn35azr3HpfTCCy+cM6Z3795avHixOnfurMcee0y33XabvLy8dOjQIa1du1Z33XWX7rnnHrVq1UrVq1fX0KFD9eyzz8rLy0uLFy8ulLS6UPv27VOHDh3Uu3dvRUREqFatWsrMzNQnn3yiN954Q5GRkWrVqpUkKS4uTnPnzlWXLl00adIkBQUFmVNSnS0mJkZ9+/ZVbGys7r33Xv36669KSEgoNIVS165dtXz5csXGxuq+++7TwYMH9dxzz6lWrVpmYv1c/vd//1e333677rjjDj3yyCOqV6+ejh8/rp9//lkfffSROSd5afpfkl27dhX6QU46Pc3W2fvnrrCwMA0ePFivvvqqrrrqKnXq1En//e9/9cwzzygkJESPP/74eW1XkiIjI2UYhrZs2aKoqKgSY/v27au+ffuWGFOaz0yBAwcOmM9aPFPNmjV13XXXma+/++47/f333+ccXAIAUBHHXXPmzJGnp6f+/e9/F3mRzJAhQzRixAh98sknuuuuu8zy5cuXy9PTU+3bt9fu3bv1zDPP6JZbblHPnj0lSTNnztSaNWvUpUsX1alTR//884+Z/GnXrp0kqV+/fnrttdfUv39//fe//1Xjxo21YcMGTZ48WZ07dzbjinLrrbcqLCxMo0eP1qlTp1S9enWtWLFCGzZsKBTbuHFjLV++XK+//rqaNWumq666Ss2bN3f7vTiX7du3y263Fyq/8cYbVa1atXPWL0r79u3VoUMHPfnkk8rKylLr1q313Xff6dlnn1XTpk0VExNzXtuVZD6778cffzzncwFHjhypkSNHlhgzZ84cVa9eXaNHjy5yKvd+/fpp2rRp+vbbb12ea+3ucdu8ebP8/f3VuHHjc+0aAAAoQfXq1RUfH68nnnhCS5YsUd++fZWdna0OHTpo586dmj59uk6dOuXyW8nZv5Gcbe3atXrsscf0wAMPqFWrVvL391dGRobeeustJScnq1+/fuYsUhMnTtRnn32mf/3rX/r3v/+txo0b69ixY0pOTtbIkSPVoEEDPf7441qwYIG6dOmiiRMnqm7duvrkk080Y8YMPfLII24903jWrFnq1KmTOnTooAcffFC1a9fWn3/+qb1792rHjh165513LvxgArgwBgzDMAxJxooVK8zXb7/9tiHJqFy5ssvi6elp9OzZ0zAMw0hLSzPq169vjBkzxtixY4exbt06IyIiwmjbtq2Rn59fTnuCsjRv3jxDkrF169YS42666SYjIiLCpezkyZPGSy+9ZNxyyy2Gr6+vUaVKFaNBgwbGkCFDjJ9++smM27hxoxEeHm5UqlTJqFmzpvHwww8bO3bsMCQZ8+bNM+P69+9vVK5cuVDbzz77rHGuUzszM9OYNGmSceeddxq1a9c2vL29jcqVKxtNmjQxJk2aZPz9998u8Xv27DHat29v+Pr6GjVq1DAGDhxofPDBB4YkY+3atWZcfn6+kZCQYFx77bWGr6+v0bx5c2PNmjVGREREoePxwgsvGPXq1TN8fHyMhg0bGrNnzy6y75KMYcOGFbkfqampxoABA4zatWsbXl5eRs2aNY1WrVoZkyZNOq/+F6XgPS9umT179jn7WtLnJi8vz5g6dapxww03GF5eXkZAQIDRt29f4+DBgy5xERERxk033VRiX8/ebr169YzY2FiX8tTUVEOS8eKLL5ZYf9iwYS7vRWk+MwVtFLc88MADLm0988wzRkBAgPHPP/+4vX8AAOuzwrjryJEjhre3t3H33XcXG5OZmWn4+fkZ3bp1c9nm9u3bjW7duhlVqlQxqlatavTp08c4fPiwWW/Tpk3GPffcY9StW9fw8fEx/P39jYiICOPDDz902f7Ro0eNoUOHGrVq1TI8PT2NunXrGvHx8YW+d+vWrWv079/fpezHH380oqKijGrVqhk1a9Y0hg8fbnzyySeFxlB//vmncd999xlXX321YbPZXI6Ju+9FUQqORXHLqlWrDMMoeXxTsI0jR44UWpednW08+eSTRt26dQ0vLy+jVq1axiOPPGJkZmYWOjZdunQpsa9ncjqdRpUqVYyEhASX8rVr1xqSjHfeeafE+l26dDHq1q1rGIZhfPvtt4YkIy4urtj4H374wZBkDB8+3DAM94+bYZwev9etW9esCwAAzq2kcWp2drZRp04do379+sapU6fO+RvJ2eOvsx08eNB4+umnjdatWxsOh8Pw9PQ0qlatarRo0cJ49dVXjVOnThWKHzBggOFwOAwvLy8jODjY6Nmzp8s48tdffzWio6MNf39/w8vLywgLCzNefPFFIy8vz4w51+9H3377rdGzZ08jMDDQ8PLyMhwOh3HnnXcaM2fOLMWRBFBWbIZhGBclg1jB2Ww2rVixQnfffbckadmyZXrggQe0e/dueXh4uMRWqVJFDodDzzzzjD777DNt27bNXHfo0CGFhIRo06ZNatmy5aXcBeCS+vLLL9WmTRutXbu2yCmnUP7+85//6Pnnn9dvv/1W7DNjylteXp6uv/56RUdHl/i8JgAAgIpk+PDh+uKLL7R79+5SPR/wUvviiy8UFRWl3bt3q0GDBuXdHQAAAAAXAc/4K0bTpk2Vl5enjIwMXX/99S6Lw+GQJP3999+FkoIFr8+cxxkAysOwYcNkt9vN5/1cjhYtWqQTJ05ozJgx5d0VAACAi+bpp5/Wb7/9pvfee6+8u1KiSZMmacCAAST9AAAAAAu5ohN/J06cUEpKilJSUiRJqampSklJ0YEDB3TDDTfogQceUL9+/bR8+XKlpqZq69atmjp1qvkMvy5dumjr1q2aOHGifvrpJ+3YsUMPPfSQ6tatW6qHtAJAWfD19dXChQvl4+NT3l0pVn5+vhYvXqyrr766vLsCAABw0RQ8Vzo7O7u8u1KszMxMRUREMOsCAAAAYDFX9FSfBVMVnq1///5KSkrSyZMnNWnSJC1YsEC//fab/P39FR4ergkTJpgPPl+6dKkSEhL0448/qlKlSgoPD9fUqVO5YhIAAAAAAAAAAACX1BV9x19kZKQMwyi0JCUlSZK8vLw0YcIEpaamKjc3V2lpaVq+fLmZ9JOk3r17a8eOHTpx4oQyMjL0wQcfkPQDAAAAAAC4zM2YMUOhoaHy9fVVs2bN9NVXX5UYv27dOjVr1ky+vr669tprNXPmzEIx7733nm688Ub5+Pjoxhtv1IoVK0rdrmEYGj9+vIKDg+Xn56fIyEjt3r37wnYWAABcMa7oxB8AAAAAAACuPMuWLVNcXJzGjh2rnTt36o477lCnTp104MCBIuNTU1PVuXNn3XHHHdq5c6f+/e9/a8SIES7P8ty0aZN69eqlmJgYffvtt4qJiVHPnj21ZcuWUrWbkJCgadOmKTExUVu3bpXD4VD79u11/PjxsjsgAADAMq64qT7z8/P1+++/q2rVqrLZbOXdHQAAcIEMw9Dx48cVHBysq66yzjVN69ev14svvqjt27crLS1NK1as0N13322uNwxDEyZM0BtvvKHMzEy1aNFCr732mm666SYzJicnR6NHj9Zbb72l7OxstW3bVjNmzNA111xjxmRmZmrEiBH68MMPJUndu3fXq6++6vLszQMHDmjYsGFas2aN/Pz8FB0drZdeekne3t5u7QvjLwAArMUK468WLVrof/7nf/T666+bZQ0bNtTdd9+tKVOmFIp/8skn9eGHH2rv3r1m2dChQ/Xtt99q06ZNkqRevXopKytLn332mRnTsWNHVa9eXW+99ZZb7RqGoeDgYMXFxenJJ5+UdHpMFxQUpKlTp2rIkCFF7k9OTo5ycnLM1/n5+frzzz/l7+/P+AsAAAso1fjLuMIcPHjQkMTCwsLCwsJiseXgwYPlPcy4qD799FNj7NixxnvvvWdIMlasWOGy/oUXXjCqVq1qvPfee8b3339v9OrVy6hVq5aRlZVlxgwdOtSoXbu2sWrVKmPHjh1GmzZtjFtuucU4deqUGdOxY0ejUaNGxsaNG42NGzcajRo1Mrp27WquP3XqlNGoUSOjTZs2xo4dO4xVq1YZwcHBxqOPPur2vjD+YmFhYWFhseZSUcdfOTk5hoeHh7F8+XKX8hEjRhj/+te/iqxzxx13GCNGjHApW758ueHp6Wnk5uYahmEYISEhxrRp01xipk2bZtSpU8ftdn/55RdDkrFjxw6XmO7duxv9+vUrdp+effbZcv88sLCwsLCwsJT94s74y1NXmKpVq0qSDh48qGrVqpVzbwAAwIXKyspSSEiI+R1vFZ06dVKnTp2KXGcYhqZPn66xY8eqR48ekqT58+crKChIS5Ys0ZAhQ+R0OjVnzhwtXLhQ7dq1kyQtWrRIISEhWr16tTp06KC9e/cqOTlZmzdvVosWLSRJs2fPVnh4uPbt26ewsDCtXLlSe/bs0cGDBxUcHCxJ+s9//qMHH3xQzz//vFvjKcZfAABYS0Uff/3xxx/Ky8tTUFCQS3lQUJDS09OLrJOenl5k/KlTp/THH3+oVq1axcYUbNOddgv+W1TMr7/+Wuw+xcfHa+TIkeZrp9OpOnXqlN34y26/+NsErMjpLO8eXDyc94B7yui8L83464pL/BVMb1CtWjV+eAIAwEKupCmMUlNTlZ6erqioKLPMx8dHERER2rhxo4YMGaLt27fr5MmTLjHBwcFq1KiRNm7cqA4dOmjTpk2y2+1m0k+SWrZsKbvdro0bNyosLEybNm1So0aNzKSfJHXo0EE5OTnavn272rRpU6h/Z081VfA8GsZfAABYS0Uff53df8MwStynouLPLndnmxcr5kw+Pj7y8fEpVM74CyhnnH/AlaeMz3t3xl8VcyJ2AACAK1hJV4KfebW4t7e3qlevXmJMYGBgoe0HBga6xJzdTvXq1eXt7V3sFfFTpkyR3W43l5CQkPPYSwAAgLIREBAgDw+PQmOZjIyMQuOeAg6Ho8h4T09P+fv7lxhTsE132nU4HJJUqr4BAACcicQfAABABVXaK8GLiikq/nxizhQfHy+n02kuBw8eLLFPAAAAl5K3t7eaNWumVatWuZSvWrVKrVq1KrJOeHh4ofiVK1eqefPm8vLyKjGmYJvutBsaGiqHw+ESk5ubq3Xr1hXbNwAAgDNdcVN9AgAAVHRnXgleq1Yts/zsq8Vzc3OVmZnpctdfRkaG+aORw+HQ4cOHC23/yJEjLtvZsmWLy/rMzEydPHmy2KvOi5tqCgAA4HIxcuRIxcTEqHnz5goPD9cbb7yhAwcOaOjQoZJOX8j022+/acGCBZKkoUOHKjExUSNHjtSgQYO0adMmzZkzR2+99Za5zccee0z/+te/NHXqVN1111364IMPtHr1am3YsMHtdm02m+Li4jR58mTVr19f9evX1+TJk1WpUiVFR0dfwiMEAAAqKu74AwAAqGDcuRK8WbNm8vLycolJS0vTrl27zJjw8HA5nU598803ZsyWLVvkdDpdYnbt2qW0tDQzZuXKlfLx8VGzZs3KdD8BAADKSq9evTR9+nRNnDhRTZo00fr16/Xpp5+qbt26kk6Pmw4cOGDGh4aG6tNPP9WXX36pJk2a6LnnntMrr7yie++914xp1aqVli5dqnnz5unmm29WUlKSli1b5vI85XO1K0lPPPGE4uLiFBsbq+bNm+u3337TypUrVbVq1UtwZAAAQEVnMwqeRFwO1q9frxdffFHbt29XWlqaVqxYobvvvrvEOjk5OZo4caIWLVqk9PR0XXPNNRo7dqwGDBjgVptZWVmy2+1yOp083BgAAAuw6nf7iRMn9PPPP0uSmjZtqmnTpqlNmzaqUaOG6tSpo6lTp2rKlCmaN2+eeSX4l19+qX379pk/Cj3yyCP6+OOPlZSUpBo1amj06NE6evSotm/fLg8PD0lSp06d9Pvvv2vWrFmSpMGDB6tu3br66KOPJEl5eXlq0qSJgoKC9OKLL+rPP//Ugw8+qLvvvluvvvqqW/ti1fcIAIArFd/tl78yf4/OMb08gP+v/H56v/g47wH3lNF5X5rv9nKd6vOvv/7SLbfcooceesjlCqmS9OzZU4cPH9acOXN0/fXXKyMjQ6dOnSrjngIAAFxa27ZtU5s2bczXI0eOlCT1799fSUlJeuKJJ5Sdna3Y2FhlZmaqRYsWha4Ef/nll+Xp6amePXsqOztbbdu2VVJSkpn0k6TFixdrxIgRioqKkiR1795diYmJ5noPDw998sknio2NVevWreXn56fo6Gi99NJLZX0IAAAAAAAAUErlesffmWw22znv+EtOTlbv3r21f/9+1ahRw63t5uTkKCcnx3ydlZWlkJAQrkoDAMAiuOL88sd7BACAtfDdfvnjjj/gMnF5/PR+cXDeA+65DO74q1DP+Pvwww/VvHlzJSQkqHbt2rrhhhs0evRoZWdnF1tnypQpstvt5hISEnIJewwAAAAAAAAAAABcGuU61Wdp7d+/Xxs2bJCvr69WrFihP/74Q7Gxsfrzzz81d+7cIuvEx8ebU2NJ/3fHHwAAAAAAAAAAAGAlFSrxl5+fL5vNpsWLF8tut0uSpk2bpvvuu0+vvfaa/Pz8CtXx8fGRj4/Ppe4qAAAAAAAAAAAAcElVqKk+a9Wqpdq1a5tJP0lq2LChDMPQoUOHyrFnAAAAAAAAAAAAQPmqUIm/1q1b6/fff9eJEyfMsh9//FFXXXWVrrnmmnLsGQAAAAAAAAAAAFC+yjXxd+LECaWkpCglJUWSlJqaqpSUFB04cEDS6efz9evXz4yPjo6Wv7+/HnroIe3Zs0fr16/XmDFjNGDAgCKn+QQAAAAAAAAAAACuFOWa+Nu2bZuaNm2qpk2bSpJGjhyppk2baty4cZKktLQ0MwkoSVWqVNGqVat07NgxNW/eXA888IC6deumV155pVz6DwAAAAAAAAAAAFwuPMuz8cjISBmGUez6pKSkQmUNGjTQqlWryrBXAAAAAAAAAAAAQMVToZ7xBwAAAAAAAAAAAKBoJP4AAAAAAAAAAAAACyDxBwAAAAAAAAAAAFgAiT8AAAAAAAAAAADAAkj8AQAAAAAAAAAAABZA4g8AAAAAAAAAAACwABJ/AAAAAAAAAAAAgAV4lncHrMZmK+8eABWHYZR3Dy4OznvAfVY573F5sU3gDzHgDuNZ6/wR5rwH3GOl8x4AAADu4Y4/AAAAAAAAAAAAwAJI/AEAAAAAAAAAAAAWQOIPAAAAAAAAAAAAsAASfwAAAAAAAAAAAIAFkPgDAAAAAAAAAAAALIDEHwAAAAAAAAAAAGABJP4AAAAAAAAAAAAACyDxBwAAAAAAAAAAAFgAiT8AAAAAAAAAAADAAkj8AQAAAAAAAAAAABZA4g8AAAAAAAAAAACwABJ/AAAAAAAAAAAAgAWQ+AMAAAAAAAAAAAAsgMQfAAAAAAAAAAAAYAEk/gAAAAAAAAAAAAALIPEHAAAAAAAAAAAAWACJPwAAAAAAAAAAAMACSPwBAAAAAAAAAAAAFkDiDwAAAAAAAAAAALAAEn8AAAAAAAAAAACABZD4AwAAAAAAAAAAACyAxB8AAAAAAAAAAABgAST+AAAAAAAAAAAAAAsg8QcAAAAAAAAAAABYAIk/AAAAAAAAAAAAwAJI/AEAAAAAAAAAAAAWQOIPAAAAAAAAAAAAsAASfwAAAAAAAAAAAIAFkPgDAAAAAAAAAAAALIDEHwAAAAAAAAAAAGABJP4AAAAAAAAAAAAACyDxBwAAAAAAAAAAAFgAiT8AAAAAAAAAAADAAkj8AQAAAAAAAAAAABZA4g8AAAAAAAAAAACwABJ/AAAAAAAAAAAAgAWQ+AMAAAAAAAAAAAAsgMQfAAAAAAAAAAAAYAEk/gAAAAAAAAAAAAALIPEHAAAAAAAAAAAAWACJPwAAAAAAAAAAAMACSPwBAAAAAAAAAAAAFkDiDwAAAAAAAAAAALAAEn8AAAAAAAAAAACABZD4AwAAAAAAAAAAACygXBN/69evV7du3RQcHCybzab333/f7bpff/21PD091aRJkzLrHwAAAAAAAAAAAFBRlGvi76+//tItt9yixMTEUtVzOp3q16+f2rZtW0Y9AwAAAAAAgBVlZmYqJiZGdrtddrtdMTExOnbsWIl1DMPQ+PHjFRwcLD8/P0VGRmr37t0uMTk5ORo+fLgCAgJUuXJlde/eXYcOHSp124899piaNWsmHx8fLngHAAClVq6Jv06dOmnSpEnq0aNHqeoNGTJE0dHRCg8PL6OeAQAAAAAAwIqio6OVkpKi5ORkJScnKyUlRTExMSXWSUhI0LRp05SYmKitW7fK4XCoffv2On78uBkTFxenFStWaOnSpdqwYYNOnDihrl27Ki8vr1RtG4ahAQMGqFevXhd3xwEAwBXBs7w7UFrz5s3TL7/8okWLFmnSpEnnjM/JyVFOTo75Oisrqyy7BwAAAAAAgMvU3r17lZycrM2bN6tFixaSpNmzZys8PFz79u1TWFhYoTqGYWj69OkaO3asefH6/PnzFRQUpCVLlmjIkCFyOp2aM2eOFi5cqHbt2kmSFi1apJCQEK1evVodOnRwu+1XXnlFknTkyBF99913bu0Xv38BAIAC5XrHX2n99NNPeuqpp7R48WJ5erqXs5wyZYo5fYLdbldISEgZ9xIAAAAAAACXo02bNslut5uJN0lq2bKl7Ha7Nm7cWGSd1NRUpaenKyoqyizz8fFRRESEWWf79u06efKkS0xwcLAaNWpkxpxP2+7i9y8AAFCgwiT+8vLyFB0drQkTJuiGG25wu158fLycTqe5HDx4sAx7CQAAAAAAgMtVenq6AgMDC5UHBgYqPT292DqSFBQU5FIeFBRkrktPT5e3t7eqV69eYkxp23YXv38BAIACFWaqz+PHj2vbtm3auXOnHn30UUlSfn6+DMOQp6enVq5cqTvvvLNQPR8fH/n4+Fzq7gIAAAAAAOASGT9+vCZMmFBizNatWyVJNput0DrDMIosP9PZ692pc3bM+bZ9Lvz+BQAAClSYxF+1atX0/fffu5TNmDFDa9as0bvvvqvQ0NBy6hkAAAAAAADK06OPPqrevXuXGFOvXj199913Onz4cKF1R44cKXRHXwGHwyHp9B17tWrVMsszMjLMOg6HQ7m5ucrMzHS56y8jI0OtWrUyY0rbNgAAQGmVa+LvxIkT+vnnn83XqampSklJUY0aNVSnTh3Fx8frt99+04IFC3TVVVepUaNGLvUDAwPl6+tbqBwAAAAAAABXjoCAAAUEBJwzLjw8XE6nU998841uu+02SdKWLVvkdDrNBN3ZQkND5XA4tGrVKjVt2lSSlJubq3Xr1mnq1KmSpGbNmsnLy0urVq1Sz549JUlpaWnatWuXEhISzrttAACA0irXxN+2bdvUpk0b8/XIkSMlSf3791dSUpLS0tJ04MCB8uoeAAAAAAAALKRhw4bq2LGjBg0apFmzZkmSBg8erK5duyosLMyMa9CggaZMmaJ77rlHNptNcXFxmjx5surXr6/69etr8uTJqlSpkqKjoyVJdrtdAwcO1KhRo+Tv768aNWpo9OjRaty4sdq1a1eqtn/++WedOHFC6enpys7OVkpKiiTpxhtvlLe396U4TAAAoAIr18RfZGSkDMModn1SUlKJ9cePH6/x48df3E4BAAAAAADAshYvXqwRI0YoKipKktS9e3clJia6xOzbt09Op9N8/cQTTyg7O1uxsbHKzMxUixYttHLlSlWtWtWMefnll+Xp6amePXsqOztbbdu2VVJSkjw8PErV9sMPP6x169aZrwvuMkxNTVW9evUuzkEAAACWZTNKyrxZUFZWlux2u5xOp6pVq3bRt3+Bz2IGrihW+evDeQ+4ryzO+7L+bseFK/Px1wT+EAPuMJ61yOBLnPeAu8rqvGf8dfkr8/eIfwgD7rHKj18S5z3grjI670vz3X5VmfQAAAAAAAAAAAAAwCVF4g8AAAAAAAAAAACwABJ/AAAAAAAAAAAAgAWQ+AMAAAAAAAAAAAAsgMQfAAAAAAAAAAAAYAEk/gAAAAAAAAAAAAALIPEHAAAAAAAAAAAAWACJPwAAAAAAAAAAAMACSPwBAAAAAAAAAAAAFkDiDwAAAAAAAAAAALAAEn8AAAAAAAAAAACABZD4AwAAAAAAAAAAACyAxB8AAAAAAAAAAABgAST+AAAAAAAAAAAAAAsg8QcAAAAAAAAAAABYAIk/AAAAAAAAAAAAwAJI/AEAAAAAAAAAAAAWQOIPAAAAAAAAAAAAsAASfwAAAAAAAAAAAIAFkPgDAAAAAAAAAAAALIDEHwAAAAAAAAAAAGABJP4AAAAqoFOnTunpp59WaGio/Pz8dO2112rixInKz883YwzD0Pjx4xUcHCw/Pz9FRkZq9+7dLtvJycnR8OHDFRAQoMqVK6t79+46dOiQS0xmZqZiYmJkt9tlt9sVExOjY8eOXYrdBAAAAAAAQCmQ+AMAAKiApk6dqpkzZyoxMVF79+5VQkKCXnzxRb366qtmTEJCgqZNm6bExERt3bpVDodD7du31/Hjx82YuLg4rVixQkuXLtWGDRt04sQJde3aVXl5eWZMdHS0UlJSlJycrOTkZKWkpCgmJuaS7i8AAAAAAADOzbO8OwAAAIDS27Rpk+666y516dJFklSvXj299dZb2rZtm6TTd/tNnz5dY8eOVY8ePSRJ8+fPV1BQkJYsWaIhQ4bI6XRqzpw5Wrhwodq1aydJWrRokUJCQrR69Wp16NBBe/fuVXJysjZv3qwWLVpIkmbPnq3w8HDt27dPYWFh5bD3AAAAAAAAKAp3/AEAAFRAt99+u7744gv9+OOPkqRvv/1WGzZsUOfOnSVJqampSk9PV1RUlFnHx8dHERER2rhxoyRp+/btOnnypEtMcHCwGjVqZMZs2rRJdrvdTPpJUsuWLWW3282Ys+Xk5CgrK8tlAQAAAAAAQNnjjj8AAIAK6Mknn5TT6VSDBg3k4eGhvLw8Pf/88+rTp48kKT09XZIUFBTkUi8oKEi//vqrGePt7a3q1asXiimon56ersDAwELtBwYGmjFnmzJliiZMmHBhOwgAAAAAAIBS444/AACACmjZsmVatGiRlixZoh07dmj+/Pl66aWXNH/+fJc4m83m8towjEJlZzs7pqj4krYTHx8vp9NpLgcPHnR3twAAAAAAAHABuOMPAACgAhozZoyeeuop9e7dW5LUuHFj/frrr5oyZYr69+8vh8Mh6fQde7Vq1TLrZWRkmHcBOhwO5ebmKjMz0+Wuv4yMDLVq1cqMOXz4cKH2jxw5UuhuwgI+Pj7y8fG5ODsKAAAAAAAAt3HHHwAAQAX0999/66qrXIdyHh4eys/PlySFhobK4XBo1apV5vrc3FytW7fOTOo1a9ZMXl5eLjFpaWnatWuXGRMeHi6n06lvvvnGjNmyZYucTqcZAwAAAAAAgMsDd/wBAABUQN26ddPzzz+vOnXq6KabbtLOnTs1bdo0DRgwQNLp6Tnj4uI0efJk1a9fX/Xr19fkyZNVqVIlRUdHS5LsdrsGDhyoUaNGyd/fXzVq1NDo0aPVuHFjtWvXTpLUsGFDdezYUYMGDdKsWbMkSYMHD1bXrl0VFhZWPjsPAAAAAACAIpH4AwAAqIBeffVVPfPMM4qNjVVGRoaCg4M1ZMgQjRs3zox54oknlJ2drdjYWGVmZqpFixZauXKlqlatasa8/PLL8vT0VM+ePZWdna22bdsqKSlJHh4eZszixYs1YsQIRUVFSZK6d++uxMTES7ezAAAAAAAAcIvNMAyjvDtxKWVlZclut8vpdKpatWoXffs220XfJGBZVvnrw3kPuK8szvuy/m7HhSvz8dcE/hAD7jCetcjgS5z3gLvK6rxn/HX5K/P3iH8IA+6xyo9fEuc94K4yOu9L893OM/4AAAAAAAAAAAAACyDxBwAAAAAAAAAAAFgAiT8AAAAAAAAAAADAAkj8AQAAAAAAAAAAABZA4g8AAAAAAAAAAACwABJ/AAAAAAAAAAAAgAWQ+AMAAAAAAAAAAAAsgMQfAAAAAAAAAAAAYAEk/gAAAAAAAAAAAAALIPEHAAAAAAAAAAAAWACJPwAAAAAAAAAAAMACSPwBAAAAAAAAAAAAFkDiDwAAAAAAAAAAALAAEn8AAAAAAAAAAACABZD4AwAAAAAAAAAAACyAxB8AAAAAAAAAAABgAST+AAAAAAAAAAAAAAsg8QcAAAAAAAAAAABYAIk/AAAAAAAAAAAAwAJI/AEAAAAAAAAAAAAWQOIPAAAAAAAAAAAAsIByTfytX79e3bp1U3BwsGw2m95///0S45cvX6727durZs2aqlatmsLDw/X5559fms4CAAAAAAAAAAAAl7FyTfz99ddfuuWWW5SYmOhW/Pr169W+fXt9+umn2r59u9q0aaNu3bpp586dZdxTAAAAAAAAAAAA4PLmWZ6Nd+rUSZ06dXI7fvr06S6vJ0+erA8++EAfffSRmjZtWmSdnJwc5eTkmK+zsrLOq68AAAAAAAAAAADA5axCP+MvPz9fx48fV40aNYqNmTJliux2u7mEhIRcwh4CAAAAAAAAAAAAl0aFTvz95z//0V9//aWePXsWGxMfHy+n02kuBw8evIQ9BAAAAAAAAAAAAC6Ncp3q80K89dZbGj9+vD744AMFBgYWG+fj4yMfH59L2DMAAAAAAAAAAADg0quQib9ly5Zp4MCBeuedd9SuXbvy7g4AAAAAAAAAAABQ7ircVJ9vvfWWHnzwQS1ZskRdunQp7+4AAAAAAAAAAAAAl4VyvePvxIkT+vnnn83XqampSklJUY0aNVSnTh3Fx8frt99+04IFCySdTvr169dP//u//6uWLVsqPT1dkuTn5ye73V4u+wAAAAAAAAAAAABcDsr1jr9t27apadOmatq0qSRp5MiRatq0qcaNGydJSktL04EDB8z4WbNm6dSpUxo2bJhq1aplLo899li59B8AAAAAAAAAAAC4XJRr4i8yMlKGYRRakpKSJElJSUn68ssvzfgvv/yyxHgAAAAAAACgJJmZmYqJiZHdbpfdbldMTIyOHTtWYh3DMDR+/HgFBwfLz89PkZGR2r17t0tMTk6Ohg8froCAAFWuXFndu3fXoUOHStX2t99+qz59+igkJER+fn5q2LCh/vd///di7ToAALgCVLhn/AEAAAAAAADnKzo6WikpKUpOTlZycrJSUlIUExNTYp2EhARNmzZNiYmJ2rp1qxwOh9q3b6/jx4+bMXFxcVqxYoWWLl2qDRs26MSJE+ratavy8vLcbnv79u2qWbOmFi1apN27d2vs2LGKj49XYmLixT8QAADAkmyGYRjl3YlLKSsrS3a7XU6nU9WqVbvo27fZLvomAcuyyl8fznvAfWVx3pf1dzsuXJmPvybwhxhwh/GsRQZf4rwH3FVW531FHn/t3btXN954ozZv3qwWLVpIkjZv3qzw8HD98MMPCgsLK1THMAwFBwcrLi5OTz75pKTTd/cFBQVp6tSpGjJkiJxOp2rWrKmFCxeqV69ekqTff/9dISEh+vTTT9WhQ4fzaluShg0bpr1792rNmjVu72eZv0f8Qxhwj1V+/JI47wF3ldF5X5rvdu74AwAAAAAAwBVh06ZNstvtZuJNklq2bCm73a6NGzcWWSc1NVXp6emKiooyy3x8fBQREWHW2b59u06ePOkSExwcrEaNGpkx59O2JDmdTtWoUaPE/crJyVFWVpbLAgAArkwk/gAAAAAAAHBFSE9PV2BgYKHywMBApaenF1tHkoKCglzKg4KCzHXp6eny9vZW9erVS4wpbdubNm3S22+/rSFDhpS4X1OmTDGfG2i32xUSElJiPAAAsC4SfwAAAAAAAKjQxo8fL5vNVuKybds2SZKtiOnqDMMosvxMZ693p87ZMaVpe/fu3brrrrs0btw4tW/fvsR24uPj5XQ6zeXgwYMlxgMAAOvyLO8OAAAAAAAAABfi0UcfVe/evUuMqVevnr777jsdPny40LojR44UuqOvgMPhkHT6jr1atWqZ5RkZGWYdh8Oh3NxcZWZmutz1l5GRoVatWpkx7ra9Z88e3XnnnRo0aJCefvrpEvdLOj31qI+PzznjAACA9XHHHwAAAAAAACq0gIAANWjQoMTF19dX4eHhcjqd+uabb8y6W7ZskdPpNBN0ZwsNDZXD4dCqVavMstzcXK1bt86s06xZM3l5ebnEpKWladeuXWaMu23v3r1bbdq0Uf/+/fX8889fnAMEAACuGCT+AAAAAAAAcEVo2LChOnbsqEGDBmnz5s3avHmzBg0apK5duyosLMyMa9CggVasWCHp9PSccXFxmjx5slasWKFdu3bpwQcfVKVKlRQdHS1JstvtGjhwoEaNGqUvvvhCO3fuVN++fdW4cWO1a9fO7bYLkn7t27fXyJEjlZ6ervT0dB05cuQSHykAAFBRMdUnAAAAAAAArhiLFy/WiBEjFBUVJUnq3r27EhMTXWL27dsnp9Npvn7iiSeUnZ2t2NhYZWZmqkWLFlq5cqWqVq1qxrz88svy9PRUz549lZ2drbZt2yopKUkeHh5ut/3OO+/oyJEjWrx4sRYvXmyW161bV//9738v6nEAAADWZDMMwyjvTlxKWVlZstvtcjqdqlat2kXf/jme6QzgDFb568N5D7ivLM77sv5ux4Ur8/HXBP4QA+4wnrXI4Euc94C7yuq8Z/x1+Svz94h/CAPuscqPXxLnPeCuMjrvS/PdzlSfAAAAAAAAAAAAgAWQ+AMAAAAAAAAAAAAsgMQfAAAAAAAAAAAAYAEk/gAAAAAAAAAAAAALIPEHAAAAAAAAAAAAWACJPwAAAAAAAAAAAMACSPwBAAAAAAAAAAAAFkDiDwAAAAAAAAAAALAAEn8AAAAAAAAAAACABZD4AwAAAAAAAAAAACyAxB8AAAAAAAAAAABgAST+AAAAAAAAAAAAAAsg8QcAAAAAAAAAAABYAIk/AAAAAAAAAAAAwAJI/AEAAAAAAAAAAAAWQOIPAAAAAAAAAAAAsAASfwAAAAAAAAAAAIAFkPgDAAAAAAAAAAAALIDEHwAAAAAAAAAAAGABJP4AAAAAAAAAAAAACyDxBwAAAAAAAAAAAFgAiT8AAAAAAAAAAADAAkj8AQAAAAAAAAAAABZA4g8AAAAAAAAAAACwABJ/AAAAAAAAAAAAgAWQ+AMAAAAAAAAAAAAsgMQfAAAAAAAAAAAAYAEk/gAAAAAAAAAAAAALIPEHAAAAAAAAAAAAWACJPwAAAAAAAAAAAMACSPwBAAAAAAAAAAAAFkDiDwAAAAAAAAAAALAAEn8AAAAAAAAAAACABZD4AwAAAAAAAAAAACyAxB8AAAAAAAAAAABgAST+AAAAAAAAAAAAAAsg8QcAAAAAAAAAAABYAIk/AAAAAAAAAAAAwAJI/AEAAAAAAAAAAAAWQOIPAAAAAAAAAAAAsAASfwAAAAAAAAAAAIAFkPgDAAAAAAAAAAAALIDEHwAAAAAAAAAAAGAB5Zr4W79+vbp166bg4GDZbDa9//7756yzbt06NWvWTL6+vrr22ms1c+bMsu8oAAAAAAAAAAAAcJkr18TfX3/9pVtuuUWJiYluxaempqpz58664447tHPnTv373//WiBEj9N5775VxTwEAAAAAAAAAAIDLm2d5Nt6pUyd16tTJ7fiZM2eqTp06mj59uiSpYcOG2rZtm1566SXde++9ZdRLAAAAAAAAAAAA4PJXoZ7xt2nTJkVFRbmUdejQQdu2bdPJkyeLrJOTk6OsrCyXBQAAAAAAAAAAALCaCpX4S09PV1BQkEtZUFCQTp06pT/++KPIOlOmTJHdbjeXkJCQS9FVAAAAAAAAAAAA4JKqUIk/SbLZbC6vDcMosrxAfHy8nE6nuRw8eLDM+wgAAAAAAAAAAABcauX6jL/ScjgcSk9PdynLyMiQp6en/P39i6zj4+MjHx+fS9E9AAAAAAAAAAAAoNxUqDv+wsPDtWrVKpeylStXqnnz5vLy8iqnXgEAAAAAAAAAAADlr1wTfydOnFBKSopSUlIkSampqUpJSdGBAwcknZ6ms1+/fmb80KFD9euvv2rkyJHau3ev5s6dqzlz5mj06NHl0X0AAAAAAAAAAADgslGuib9t27apadOmatq0qSRp5MiRatq0qcaNGydJSktLM5OAkhQaGqpPP/1UX375pZo0aaLnnntOr7zyiu69995y6T8AAEB5+u2339S3b1/5+/urUqVKatKkibZv326uNwxD48ePV3BwsPz8/BQZGandu3e7bCMnJ0fDhw9XQECAKleurO7du+vQoUMuMZmZmYqJiZHdbpfdbldMTIyOHTt2KXYRAAAAAAAApVCuz/iLjIyUYRjFrk9KSipUFhERoR07dpRhrwAAAC5/mZmZat26tdq0aaPPPvtMgYGB+uWXX3T11VebMQkJCZo2bZqSkpJ0ww03aNKkSWrfvr327dunqlWrSpLi4uL00UcfaenSpfL399eoUaPUtWtXbd++XR4eHpKk6OhoHTp0SMnJyZKkwYMHKyYmRh999NEl328AAAAAAAAUr1wTfwAAADg/U6dOVUhIiObNm2eW1atXz/x/wzA0ffp0jR07Vj169JAkzZ8/X0FBQVqyZImGDBkip9OpOXPmaOHChWrXrp0kadGiRQoJCdHq1avVoUMH7d27V8nJydq8ebNatGghSZo9e7bCw8O1b98+hYWFFepbTk6OcnJyzNdZWVllcQgAAAAAAABwlnKd6hMAAADn58MPP1Tz5s11//33KzAwUE2bNtXs2bPN9ampqUpPT1dUVJRZ5uPjo4iICG3cuFGStH37dp08edIlJjg4WI0aNTJjNm3aJLvdbib9JKlly5ay2+1mzNmmTJliTgtqt9sVEhJyUfcdAAAAAAAARSPxBwAAUAHt379fr7/+uurXr6/PP/9cQ4cO1YgRI7RgwQJJUnp6uiQpKCjIpV5QUJC5Lj09Xd7e3qpevXqJMYGBgYXaDwwMNGPOFh8fL6fTaS4HDx68sJ0FAAAAAACAW5jqEwAAoALKz89X8+bNNXnyZElS06ZNtXv3br3++uvq16+fGWez2VzqGYZRqOxsZ8cUFV/Sdnx8fOTj4+P2vgAAAAAAAODi4I4/AACACqhWrVq68cYbXcoaNmyoAwcOSJIcDockFborLyMjw7wL0OFwKDc3V5mZmSXGHD58uFD7R44cKXQ3IQAAAAAAAMoXiT8AAIAKqHXr1tq3b59L2Y8//qi6detKkkJDQ+VwOLRq1SpzfW5urtatW6dWrVpJkpo1ayYvLy+XmLS0NO3atcuMCQ8Pl9Pp1DfffGPGbNmyRU6n04wBAAAAAADA5YGpPgEAACqgxx9/XK1atdLkyZPVs2dPffPNN3rjjTf0xhtvSDo9PWdcXJwmT56s+vXrq379+po8ebIqVaqk6OhoSZLdbtfAgQM1atQo+fv7q0aNGho9erQaN26sdu3aSTp9F2HHjh01aNAgzZo1S5I0ePBgde3aVWFhYeWz8wAAAAAAACgSiT8AAIAK6NZbb9WKFSsUHx+viRMnKjQ0VNOnT9cDDzxgxjzxxBPKzs5WbGysMjMz1aJFC61cuVJVq1Y1Y15++WV5enqqZ8+eys7OVtu2bZWUlCQPDw8zZvHixRoxYoSioqIkSd27d1diYuKl21kAAAAAAAC4xWYYhlHenbiUsrKyZLfb5XQ6Va1atYu+fZvtom8SsCyr/PXhvAfcVxbnfVl/t+PClfn4awJ/iAF3GM9aZPAlznvAXWV13jP+uvyV+XvEP4QB91jlxy+J8x5wVxmd96X5bucZfwAAAAAAAAAAAIAFkPgDAAAAAADAFSMzM1MxMTGy2+2y2+2KiYnRsWPHSqxjGIbGjx+v4OBg+fn5KTIyUrt373aJycnJ0fDhwxUQEKDKlSure/fuOnToUKnaPnr0qDp27Kjg4GD5+PgoJCREjz76qLKysi7W7gMAAIsj8QcAAAAAAIArRnR0tFJSUpScnKzk5GSlpKQoJiamxDoJCQmaNm2aEhMTtXXrVjkcDrVv317Hjx83Y+Li4rRixQotXbpUGzZs0IkTJ9S1a1fl5eW53fZVV12lu+66Sx9++KF+/PFHJSUlafXq1Ro6dOjFPxAAAMCSPMu7AwAAAAAAAMClsHfvXiUnJ2vz5s1q0aKFJGn27NkKDw/Xvn37FBYWVqiOYRiaPn26xo4dqx49ekiS5s+fr6CgIC1ZskRDhgyR0+nUnDlztHDhQrVr106StGjRIoWEhGj16tXq0KGDW21Xr15djzzyiNl23bp1FRsbqxdffLGsDw0AALAI7vgDAAAAAADAFWHTpk2y2+1m4k2SWrZsKbvdro0bNxZZJzU1Venp6YqKijLLfHx8FBERYdbZvn27Tp486RITHBysRo0amTHn0/bvv/+u5cuXKyIiosT9ysnJUVZWlssCAACuTCT+AAAAAAAAcEVIT09XYGBgofLAwEClp6cXW0eSgoKCXMqDgoLMdenp6fL29lb16tVLjHG37T59+qhSpUqqXbu2qlWrpjfffLPE/ZoyZYr53EC73a6QkJAS4wEAgHWR+AMAAAAAAECFNn78eNlsthKXbdu2SZJsNluh+oZhFFl+prPXu1Pn7Bh323755Ze1Y8cOvf/++/rll180cuTIEtuJj4+X0+k0l4MHD5YYDwAArItn/AEAAAAAAKBCe/TRR9W7d+8SY+rVq6fvvvtOhw8fLrTuyJEjhe7oK+BwOCSdvmOvVq1aZnlGRoZZx+FwKDc3V5mZmS53/WVkZKhVq1ZmjLttOxwOORwONWjQQP7+/rrjjjv0zDPPuLR/Jh8fH/n4+JS0+wAA4ArBHX8AAAAAAACo0AICAtSgQYMSF19fX4WHh8vpdOqbb74x627ZskVOp9NM0J0tNDRUDodDq1atMstyc3O1bt06s06zZs3k5eXlEpOWlqZdu3aZMefTtnT6jkDp9HP8AAAAzoU7/gAAAAAAAHBFaNiwoTp27KhBgwZp1qxZkqTBgwera9euCgsLM+MaNGigKVOm6J577pHNZlNcXJwmT56s+vXrq379+po8ebIqVaqk6OhoSZLdbtfAgQM1atQo+fv7q0aNGho9erQaN26sdu3aud32p59+qsOHD+vWW29VlSpVtGfPHj3xxBNq3bq16tWrdwmPFAAAqKhI/AEAAAAAAOCKsXjxYo0YMUJRUVGSpO7duysxMdElZt++fXI6nebrJ554QtnZ2YqNjVVmZqZatGihlStXqmrVqmbMyy+/LE9PT/Xs2VPZ2dlq27atkpKS5OHh4Xbbfn5+mj17th5//HHl5OQoJCREPXr00FNPPVUmxwIAAFiPzSiYL+AKkZWVJbvdLqfTqWrVql307Z/jmc4AzmCVvz6c94D7yuK8L+vvdly4Mh9/TeAPMeAO41mLDL7EeQ+4q6zOe8Zfl78yf4/4hzDgHqv8+CVx3gPuKqPzvjTf7TzjDwAAAAAAAAAAALAAEn8AAAAAAAAAAACABZD4AwAAAAAAAAAAACyAxB8AAAAAAAAAAABgAST+AAAAAAAAAAAAAAsg8QcAAAAAAAAAAABYAIk/AAAAAAAAAAAAwAJI/AEAAAAAAAAAAAAWQOIPAAAAAAAAAAAAsAASfwAAAAAAAAAAAIAFkPgDAAAAAAAAAAAALIDEHwAAAAAAAAAAAGABJP4AAAAAAAAAAAAACyDxBwAAAAAAAAAAAFgAiT8AAAAAAAAAAADAAkj8AQAAAAAAAAAAABZA4g8AAAAAAAAAAACwABJ/AAAAAAAAAAAAgAWQ+AMAAAAAAAAAAAAsgMQfAAAAAAAAAAAAYAEk/gAAAC6Cb775Rnl5eeZrwzBc1ufk5Ojtt9++1N0CAAAAAADAFaRUib+EhARlZ2ebr9evX6+cnBzz9fHjxxUbG3vxegcAAFBBhIeH6+jRo+Zru92u/fv3m6+PHTumPn36lEfXAAAAAAAAcIUoVeIvPj5ex48fN1937dpVv/32m/n677//1qxZsy5e7wAAACqIs+/wO/t1cWUAAAAAAADAxVKqxJ87P2gBAACgaDabrby7AAAAAAAAAAvjGX8AAAAAAAAAAACABXiWdwcAAACsYs+ePUpPT5d0emaEH374QSdOnJAk/fHHH+XZNQAAAAAAAFwBSp34e/PNN1WlShVJ0qlTp5SUlKSAgABJcnn+HwAAwJWmbdu2LlOhd+3aVdLpKT4Nw2CqTwAAAAAAAJSpUiX+6tSpo9mzZ5uvHQ6HFi5cWCgGAADgSpOamlreXQAAAKiQ8vPzddVVhZ9Gk5+fr0OHDvFbEwAAQCmUKvH33//+t4y6AQAAULHVrVv3nDEpKSluxQEAAFwJsrKy9PDDD+ujjz5StWrVNHToUI0bN04eHh6SpCNHjig0NFR5eXnl3FMAAICKo/DlVAAAALhonE6nZsyYof/5n/9Rs2bNyrs7AAAAl41nnnlG3377rRYuXKjnn39e8+fP11133aXc3Fwz5sxp1AEAAHBupUr8bdmyRZ999plL2YIFCxQaGqrAwEANHjxYOTk5F7WDAAAAFdGaNWvUt29f1apVS6+++qo6d+6sbdu2lXe3AAAALhvvv/++Zs2apfvuu08PP/ywtm/frj/++EPdunUzf1/iGckAAAClU6rE3/jx4/Xdd9+Zr7///nsNHDhQ7dq101NPPaWPPvpIU6ZMueidBAAAqAgOHTqkSZMm6dprr1WfPn1UvXp1nTx5Uu+9954mTZqkpk2blncXAQAALht//PGHyzTo/v7+WrVqlY4fP67OnTvr77//LsfeAQAAVEylSvylpKSobdu25uulS5eqRYsWmj17tkaOHKlXXnlFb7/99kXvJAAAwOWuc+fOuvHGG7Vnzx69+uqr+v333/Xqq6+Wd7cAAAAuWyEhIdq7d69LWdWqVbVy5UplZ2frnnvuKaeeAQAAVFylSvxlZmYqKCjIfL1u3Tp17NjRfH3rrbfq4MGDperAjBkzFBoaKl9fXzVr1kxfffVVifGLFy/WLbfcokqVKqlWrVp66KGHdPTo0VK1CQAAcLGtXLlSDz/8sCZMmKAuXbrIw8OjvLsEAABwWYuKitK8efMKlVepUkWff/65fH19y6FXAAAAFVupEn9BQUFKTU2VJOXm5mrHjh0KDw831x8/flxeXl5ub2/ZsmWKi4vT2LFjtXPnTt1xxx3q1KmTDhw4UGT8hg0b1K9fPw0cOFC7d+/WO++8o61bt+rhhx8uzW4AAABcdF999ZWOHz+u5s2bq0WLFkpMTNSRI0fKu1sAAACXrQkTJmj8+PFFrqtatapWr16tBQsWXNpOAQAAVHClSvx17NhRTz31lL766ivFx8erUqVKuuOOO8z13333na677jq3tzdt2jQNHDhQDz/8sBo2bKjp06crJCREr7/+epHxmzdvVr169TRixAiFhobq9ttv15AhQ7Rt27Zi28jJyVFWVpbLAgAAcLGFh4dr9uzZSktL05AhQ7R06VLVrl1b+fn55rNqAAAA8H+qV6+um266qch16enpio+P52JvAACAUipV4m/SpEny8PBQRESEZs+erTfeeEPe3t7m+rlz5yoqKsqtbeXm5mr79u2F4qOiorRx48Yi67Rq1UqHDh3Sp59+KsMwdPjwYb377rvq0qVLse1MmTJFdrvdXEJCQtzqHwAAwPmoVKmSBgwYoA0bNuj777/XqFGj9MILLygwMFDdu3cv7+4BAABcNo4dO6YHHnhANWvWVHBwsF555RXl5+dr3Lhxuvbaa7V582bNnTu3vLsJAABQoZQq8VezZk199dVXyszMVGZmpnr06OGy/p133il2ioaz/fHHH8rLy3N5ZqB0ejrR9PT0Iuu0atVKixcvVq9eveTt7S2Hw6Grr75ar776arHtxMfHy+l0mktpn0EIAABwvsLCwpSQkKBDhw5p6dKlstls5d0lAACAy8a///1vrV+/Xv3791eNGjX0+OOPq2vXrtqwYYM+++wzbd26VX369CnvbgIAAFQonqUJHjBggFtxpbka6+wfwAzDKPZHsT179mjEiBEaN26cOnTooLS0NI0ZM0ZDhw7VnDlziqzj4+MjHx8ft/sDAABwPtwZJ/n7+1+CngAAAFQMn3zyiebNm6d27dopNjZW119/vW644QZNnz69vLsGAABQYZUq8ZeUlKS6deuqadOmMgzjghoOCAiQh4dHobv7MjIyCt0FWGDKlClq3bq1xowZI0m6+eabVblyZd1xxx2aNGmSatWqdUF9AgAAOF/ujJO44w8AAOD//P7777rxxhslSddee618fX15ph8AAMAFKlXib+jQoVq6dKn279+vAQMGqG/fvqpRo8Z5Nezt7a1mzZpp1apVuueee8zyVatW6a677iqyzt9//y1PT9cue3h4SNIFJyIBAAAuxMUcJwEAAFwJ8vPz5eXlZb728PBQ5cqVy7FHAAAAFV+pnvE3Y8YMpaWl6cknn9RHH32kkJAQ9ezZU59//vl5Jd5GjhypN998U3PnztXevXv1+OOP68CBAxo6dKik08/n69evnxnfrVs3LV++XK+//rr279+vr7/+WiNGjNBtt92m4ODgUrcPAABwsVzscRIAAIDVGYahBx98UD169FCPHj30zz//aOjQoebrggUAAADuK9Udf9LpZ+b16dNHffr00a+//qqkpCTFxsbq5MmT2rNnj6pUqeL2tnr16qWjR49q4sSJSktLU6NGjfTpp5+qbt26kqS0tDQdOHDAjH/wwQd1/PhxJSYmatSoUbr66qt15513aurUqaXdDQAAgIvuYo6TAAAArK5///4ur/v27VtOPQEAALCOUif+zmSz2WSz2WQYhvLz889rG7GxsYqNjS1yXVJSUqGy4cOHa/jw4efVFgAAwKVyMcZJAAAAVjZv3rzy7gIAAIDllGqqT0nKycnRW2+9pfbt2yssLEzff/+9EhMTdeDAAa5iBwAAVzTGSQAAAAAAAChPpbrjLzY2VkuXLlWdOnX00EMPaenSpfL39y+rvgEAAFQYjJMAAAAAAABQ3kqV+Js5c6bq1Kmj0NBQrVu3TuvWrSsybvny5RelcwAAABUF4yQAAAAAAACUt1Il/vr16yebzVZWfQEAAKiwGCcBAAAAAACgvJUq8ZeUlFRG3QAAAKjYGCcBAAAAAACgvF1V3h0AAAAAAAAAAAAAcOFI/AEAAAAAAAAAAAAWQOIPAAAAAAAAAAAAsAASfwAAAAAAAAAAAIAFkPgDAAAAAAAAAAAALIDEHwAAAAAAAAAAAGABJP4AAAAAAAAAAAAACyDxBwAAAAAAAAAAAFgAiT8AAAAAAAAAAADAAkj8AQAAAAAAAAAAABZA4g8AAAAAAAAAAACwABJ/AAAAAAAAAAAAgAWQ+AMAAAAAAAAAAAAsgMQfAAAAAAAAAAAAYAEk/gAAAAAAAAAAAAALIPEHAAAAAACAK0ZmZqZiYmJkt9tlt9sVExOjY8eOlVjHMAyNHz9ewcHB8vPzU2RkpHbv3u0Sk5OTo+HDhysgIECVK1dW9+7ddejQofNu++jRo7rmmmtks9nO2T8AAIACJP4AAAAAAABwxYiOjlZKSoqSk5OVnJyslJQUxcTElFgnISFB06ZNU2JiorZu3SqHw6H27dvr+PHjZkxcXJxWrFihpUuXasOGDTpx4oS6du2qvLy882p74MCBuvnmmy/OTgMAgCuGZ3l3AAAAAAAAALgU9u7dq+TkZG3evFktWrSQJM2ePVvh4eHat2+fwsLCCtUxDEPTp0/X2LFj1aNHD0nS/PnzFRQUpCVLlmjIkCFyOp2aM2eOFi5cqHbt2kmSFi1apJCQEK1evVodOnQoVduvv/66jh07pnHjxumzzz4r68MCAAAshDv+AAAAAAAAcEXYtGmT7Ha7mXiTpJYtW8put2vjxo1F1klNTVV6erqioqLMMh8fH0VERJh1tm/frpMnT7rEBAcHq1GjRmaMu23v2bNHEydO1IIFC3TVVe79dJeTk6OsrCyXBQAAXJlI/AEAAAAAAOCKkJ6ersDAwELlgYGBSk9PL7aOJAUFBbmUBwUFmevS09Pl7e2t6tWrlxhzrrZzcnLUp08fvfjii6pTp47b+zVlyhTzuYF2u10hISFu1wUAANZC4g8AAAAAAAAV2vjx42Wz2Upctm3bJkmy2WyF6huGUWT5mc5e706ds2PO1XZ8fLwaNmyovn37lrjds8XHx8vpdJrLwYMHS1UfAABYB8/4AwAAAAAAQIX26KOPqnfv3iXG1KtXT999950OHz5caN2RI0cK3dFXwOFwSDp9x16tWrXM8oyMDLOOw+FQbm6uMjMzXe76y8jIUKtWrcyYc7W9Zs0aff/993r33XclnU4KSlJAQIDGjh2rCRMmFNlHHx8f+fj4lLj/AADgykDiDwAAAAAAABVaQECAAgICzhkXHh4up9Opb775RrfddpskacuWLXI6nWaC7myhoaFyOBxatWqVmjZtKknKzc3VunXrNHXqVElSs2bN5OXlpVWrVqlnz56SpLS0NO3atUsJCQlut/3ee+8pOzvbbHvr1q0aMGCAvvrqK1133XXnc2gAAMAVhsQfAAAAAAAArggNGzZUx44dNWjQIM2aNUuSNHjwYHXt2lVhYWFmXIMGDTRlyhTdc889stlsiouL0+TJk1W/fn3Vr19fkydPVqVKlRQdHS1JstvtGjhwoEaNGiV/f3/VqFFDo0ePVuPGjdWuXTu32z47uffHH3+Yda+++uoyPTYAAMAaSPwBAAAAAADgirF48WKNGDFCUVFRkqTu3bsrMTHRJWbfvn1yOp3m6yeeeELZ2dmKjY1VZmamWrRooZUrV6pq1apmzMsvvyxPT0/17NlT2dnZatu2rZKSkuTh4VGqtgEAAC6EzSiYLPwKkZWVJbvdLqfTqWrVql307Z/jmc4AzmCVvz6c94D7yuK8L+vvdly4Mh9/TeAPMeAO41mLDL7EeQ+4q6zOe8Zfl78yf4/4hzDgHqv8+CVx3gPuKqPzvjTf7VeVSQ8AAAAAAAAAAAAAXFIk/gAAAAAAAAAAAAALIPEHAAAAAAAAAAAAWACJPwAAAAAAAAAAAMACSPwBAAAAAAAAAAAAFkDiDwAAAAAAAAAAALAAEn8AAAAWMGXKFNlsNsXFxZllhmFo/PjxCg4Olp+fnyIjI7V7926Xejk5ORo+fLgCAgJUuXJlde/eXYcOHXKJyczMVExMjOx2u+x2u2JiYnTs2LFLsFcAAAAAAAAoDRJ/AAAAFdzWrVv1xhtv6Oabb3YpT0hI0LRp05SYmKitW7fK4XCoffv2On78uBkTFxenFStWaOnSpdqwYYNOnDihrl27Ki8vz4yJjo5WSkqKkpOTlZycrJSUFMXExFyy/QMAAAAAAIB7SPwBAABUYCdOnNADDzyg2bNnq3r16ma5YRiaPn26xo4dqx49eqhRo0aaP3++/v77by1ZskSS5HQ6NWfOHP3nP/9Ru3bt1LRpUy1atEjff/+9Vq9eLUnau3evkpOT9eabbyo8PFzh4eGaPXu2Pv74Y+3bt69c9hkAAAAAAABFI/EHAABQgQ0bNkxdunRRu3btXMpTU1OVnp6uqKgos8zHx0cRERHauHGjJGn79u06efKkS0xwcLAaNWpkxmzatEl2u10tWrQwY1q2bCm73W7GnC0nJ0dZWVkuCwAAAAAAAMqeZ3l3AAAAAOdn6dKl2rFjh7Zu3VpoXXp6uiQpKCjIpTwoKEi//vqrGePt7e1yp2BBTEH99PR0BQYGFtp+YGCgGXO2KVOmaMKECaXfIQAAAAAAAFwQ7vgDAACogA4ePKjHHntMixYtkq+vb7FxNpvN5bVhGIXKznZ2TFHxJW0nPj5eTqfTXA4ePFhiewAAAAAAALg4SPwBAABUQNu3b1dGRoaaNWsmT09PeXp6at26dXrllVfk6elp3ul39l15GRkZ5jqHw6Hc3FxlZmaWGHP48OFC7R85cqTQ3YQFfHx8VK1aNZcFAAAAAAAAZY/EHwAAQAXUtm1bff/990pJSTGX5s2b64EHHlBKSoquvfZaORwOrVq1yqyTm5urdevWqVWrVpKkZs2aycvLyyUmLS1Nu3btMmPCw8PldDr1zTffmDFbtmyR0+k0YwAAAAAAAHB54Bl/AAAAFVDVqlXVqFEjl7LKlSvL39/fLI+Li9PkyZNVv3591a9fX5MnT1alSpUUHR0tSbLb7Ro4cKBGjRolf39/1ahRQ6NHj1bjxo3Vrl07SVLDhg3VsWNHDRo0SLNmzZIkDR48WF27dlVYWNgl3GMAAAAAAACcC4k/AAAAi3riiSeUnZ2t2NhYZWZmqkWLFlq5cqWqVq1qxrz88svy9PRUz549lZ2drbZt2yopKUkeHh5mzOLFizVixAhFRUVJkrp3767ExMRLvj8AAAAAAAAomc0wDKO8O3EpZWVlyW63y+l0lsnzZmy2i75JwLKs8teH8x5wX1mc92X93Y4LV+bjrwn8IQbcYTxrkcGXOO8Bd5XVec/46/JX5u8R/xAG3GOVH78kznvAXWV03pfmu51n/AEAAAAAAAAAAAAWQOIPAAAAAAAAAAAAsAASfwAAAAAAAAAAAIAFkPgDAAAAAAAAAAAALIDEHwAAAAAAAAAAAGAB5Z74mzFjhkJDQ+Xr66tmzZrpq6++KjE+JydHY8eOVd26deXj46PrrrtOc+fOvUS9BQAAAAAAAAAAAC5PnuXZ+LJlyxQXF6cZM2aodevWmjVrljp16qQ9e/aoTp06Rdbp2bOnDh8+rDlz5uj6669XRkaGTp06dYl7DgAAAAAAAAAAAFxeyjXxN23aNA0cOFAPP/ywJGn69On6/PPP9frrr2vKlCmF4pOTk7Vu3Trt379fNWrUkCTVq1fvUnYZAAAAAAAAAAAAuCyV21Sfubm52r59u6KiolzKo6KitHHjxiLrfPjhh2revLkSEhJUu3Zt3XDDDRo9erSys7OLbScnJ0dZWVkuCwAAAAAAAAAAAGA15XbH3x9//KG8vDwFBQW5lAcFBSk9Pb3IOvv379eGDRvk6+urFStW6I8//lBsbKz+/PPPYp/zN2XKFE2YMOGi9x8AAAAAAAAAAAC4nJTbHX8FbDaby2vDMAqVFcjPz5fNZtPixYt12223qXPnzpo2bZqSkpKKvesvPj5eTqfTXA4ePHjR9wEAAAAAAAAAAAAob+V2x19AQIA8PDwK3d2XkZFR6C7AArVq1VLt2rVlt9vNsoYNG8owDB06dEj169cvVMfHx0c+Pj4Xt/MAAAAAAAAAAADAZabc7vjz9vZWs2bNtGrVKpfyVatWqVWrVkXWad26tX7//XedOHHCLPvxxx911VVX6ZprrinT/gIAAAAAAAAAAACXs3Kd6nPkyJF68803NXfuXO3du1ePP/64Dhw4oKFDh0o6PU1nv379zPjo6Gj5+/vroYce0p49e7R+/XqNGTNGAwYMkJ+fX3ntBgAAAAAAAAAAAFDuym2qT0nq1auXjh49qokTJyotLU2NGjXSp59+qrp160qS0tLSdODAATO+SpUqWrVqlYYPH67mzZvL399fPXv21KRJk8prFwAAAAAAAAAAAIDLQrkm/iQpNjZWsbGxRa5LSkoqVNagQYNC04MCAAAAAAAAAAAAV7pyneoTAAAAAAAAAAAAwMVB4g8AAAAAAAAAAACwABJ/AAAAAAAAAAAAgAWQ+AMAAAAAAAAAAAAsgMQfAAAAAAAAAAAAYAEk/gAAAAAAAAAAAAALIPEHAAAAAAAAAAAAWACJPwAAAAAAAAAAAMACSPwBAAAAAAAAAAAAFkDiDwAAAAAAAAAAALAAEn8AAAAAAAAAAACABZD4AwAAAAAAAAAAACyAxB8AAAAAAAAAAABgAST+AAAAAAAAAAAAAAsg8QcAAAAAAAAAAABYAIk/AAAAAAAAAAAAwAJI/AEAAAAAAAAAAAAWQOIPAAAAAAAAAAAAsAASfwAAAAAAAAAAAIAFkPgDAAAAAAAAAAAALIDEHwAAAAAAAAAAAGABJP4AAAAAAAAAAAAACyDxBwAAAAAAAAAAAFgAiT8AAAAAAAAAAADAAkj8AQAAAAAAAAAAABZA4g8AAAAAAAAAAACwABJ/AAAAAAAAAAAAgAWQ+AMAAAAAAAAAAAAsgMQfAAAAAAAAAAAAYAEk/gAAAAAAAAAAAAALIPEHAAAAAAAAAAAAWACJPwAAAAAAAFwxMjMzFRMTI7vdLrvdrpiYGB07dqzEOoZhaPz48QoODpafn58iIyO1e/dul5icnBwNHz5cAQEBqly5srp3765Dhw6Vum2bzVZomTlz5sXYdQAAcAUg8QcAAAAAAIArRnR0tFJSUpScnKzk5GSlpKQoJiamxDoJCQmaNm2aEhMTtXXrVjkcDrVv317Hjx83Y+Li4rRixQotXbpUGzZs0IkTJ9S1a1fl5eWVuu158+YpLS3NXPr373/xDgAAALA0z/LuAAAAAAAAAHAp7N27V8nJydq8ebNatGghSZo9e7bCw8O1b98+hYWFFapjGIamT5+usWPHqkePHpKk+fPnKygoSEuWLNGQIUPkdDo1Z84cLVy4UO3atZMkLVq0SCEhIVq9erU6dOhQqravvvpqORwOt/crJydHOTk55uusrKzSHxwAAGAJ3PEHAAAAAACAK8KmTZtkt9vNxJsktWzZUna7XRs3biyyTmpqqtLT0xUVFWWW+fj4KCIiwqyzfft2nTx50iUmODhYjRo1MmNK0/ajjz6qgIAA3XrrrZo5c6by8/NL3K8pU6aY04fa7XaFhIS4eUQAAIDVkPgDAAAAAADAFSE9PV2BgYGFygMDA5Wenl5sHUkKCgpyKQ8KCjLXpaeny9vbW9WrVy8xxp22n3vuOb3zzjtavXq1evfurVGjRmny5Mkl7ld8fLycTqe5HDx4sMR4AABgXUz1CQAAAAAAgApt/PjxmjBhQokxW7dulSTZbLZC6wzDKLL8TGevd6fO2THutP3000+b/9+kSRNJ0sSJE13Kz+bj4yMfH58S+wIAAK4MJP4AAAAAAABQoT366KPq3bt3iTH16tXTd999p8OHDxdad+TIkUJ39BUoeNZeenq6atWqZZZnZGSYdRwOh3Jzc5WZmely119GRoZatWplxpS2ben0dKBZWVk6fPhwiXEAAAASU30CAAAAAACgggsICFCDBg1KXHx9fRUeHi6n06lvvvnGrLtlyxY5nU4zQXe20NBQORwOrVq1yizLzc3VunXrzDrNmjWTl5eXS0xaWpp27dplxpxP25K0c+dO+fr66uqrrz6vYwMAAK4s3PEHAAAAAACAK0LDhg3VsWNHDRo0SLNmzZIkDR48WF27dlVYWJgZ16BBA02ZMkX33HOPbDab4uLiNHnyZNWvX1/169fX5MmTValSJUVHR0uS7Ha7Bg4cqFGjRsnf3181atTQ6NGj1bhxY7Vr187ttj/66COlp6crPDxcfn5+Wrt2rcaOHavBgwczlScAAHALiT8AAAAAAABcMRYvXqwRI0YoKipKktS9e3clJia6xOzbt09Op9N8/cQTTyg7O1uxsbHKzMxUixYttHLlSlWtWtWMefnll+Xp6amePXsqOztbbdu2VVJSkjw8PNxu28vLSzNmzNDIkSOVn5+va6+9VhMnTtSwYcPK5FgAAADrsRmGYZR3Jy6lrKws2e12OZ1OVatW7aJv/xzPdAZwBqv89eG8B9xXFud9WX+348KV+fhrAn+IAXcYz1pk8CXOe8BdZXXeM/66/JX5e8Q/hAH3WOXHL4nzHnBXGZ33pflu5xl/AAAAAAAAAAAAgAWQ+AMAAAAAAAAAAAAsgMQfAAAAAAAAAAAAYAEk/gAAAAAAAAAAAAALIPEHAAAAAAAAAAAAWACJPwAAAAAAAAAAAMACSPwBAAAAAAAAAAAAFkDiDwAAAAAAAAAAALAAEn8AAAAAAAAAAACABZD4AwAAAAAAAAAAACyAxB8AAAAAAAAAAABgAST+AAAAAAAAAAAAAAso98TfjBkzFBoaKl9fXzVr1kxfffWVW/W+/vpreXp6qkmTJmXbQQAAAAAAAAAAAKACKNfE37JlyxQXF6exY8dq587/1969h1VV53sc/2zlquJORdhSeDmlqIM1hoXYcbQRwYz0jM1QQ4E05iWvRI6XMZNyBmZsUo7Hx8ZxMLS8daY81alDoqWjCWoMjJcccxoN9QHRETdYBgrr/OHDftpyEVTcm8X79Tz7edprfdf6/da29eUL373WytfQoUP1yCOPqLCwsMHt7Ha7EhISNGLEiNs0UwAAAAAAAAAAAMC9ubTxt3TpUk2YMEHPPvus+vXrp/T0dAUHB+v1119vcLvJkycrLi5OERER1x2joqJCZWVlTi8AAAAAAAAAAADAbFzW+KusrFReXp6ioqKclkdFRWnPnj31bvfGG2/oq6++0qJFixo1TlpamqxWq+MVHBx8U/MGAAAAAAAAAAAA3JHLGn/nzp1TVVWVAgMDnZYHBgaquLi4zm2OHTumefPmaf369fLw8GjUOPPnz5fdbne8Tp48edNzBwAAAAAAAAAAANxN47pnzchisTi9Nwyj1jJJqqqqUlxcnF5++WX16dOn0fv39vaWt7f3Tc8TAAAAAAAAAAAAcGcua/z5+/urbdu2ta7uKykpqXUVoCSVl5fr888/V35+vqZPny5Jqq6ulmEY8vDw0NatW/XjH//4tswdAAAAAAAAAAAAcDcuu9Wnl5eXwsLClJ2d7bQ8OztbQ4YMqRXfsWNHHTx4UAUFBY7XlClTFBISooKCAoWHh9+uqQMAAAAAAAAAAABux6W3+kxOTlZ8fLwGDRqkiIgI/fGPf1RhYaGmTJki6erz+U6fPq1169apTZs2Cg0Nddo+ICBAPj4+tZYDAAAAAAAAAAAArY1LG39PPPGE/vWvf+mVV15RUVGRQkND9dFHH6lHjx6SpKKiIhUWFrpyigAAAAAAAAAAAECLYDEMw3D1JG6nsrIyWa1W2e12dezY8Zbv32K55bsETMss2YfzHmi85jjvm/tnO25es9dfL5OIgcYwFpmk+BLnPdBYzXXeU3+5v2b/N+IXYaBxzPLHL4nzHmisZjrvm/Kz3WXP+AMAAAAAAAAAAABw69D4AwAAAAAAAAAAAEyAxh8AAAAAAAAAAABgAjT+AAAAAAAAAAAAABOg8QcAAAAAAAAAAACYAI0/AAAAAAAAAAAAwARo/AEAAAAAAAAAAAAmQOMPAAAAAAAAAAAAMAEafwAAAAAAAAAAAIAJ0PgDAAAAAAAAAAAATIDGHwAAAAAAAAAAAGACNP4AAAAAAAAAAAAAE6DxBwAAAAAAAAAAAJgAjT8AAIAWKC0tTQ888ID8/PwUEBCg//iP/9DRo0edYgzDUEpKioKCguTr66vhw4fr8OHDTjEVFRWaMWOG/P391b59e40ZM0anTp1yiiktLVV8fLysVqusVqvi4+N14cKF5j5EAAAAAAAANBGNPwAAgBZo586dmjZtmnJzc5Wdna0rV64oKipK33zzjSNmyZIlWrp0qVasWKH9+/fLZrNp5MiRKi8vd8QkJSVpy5Yt2rRpk3bv3q2LFy8qJiZGVVVVjpi4uDgVFBQoKytLWVlZKigoUHx8/G09XgAAAAAAAFyfh6snAAAAgKbLyspyev/GG28oICBAeXl5+tGPfiTDMJSenq4FCxZo3LhxkqS1a9cqMDBQGzZs0OTJk2W325WRkaE333xTkZGRkqS33npLwcHB2rZtm6Kjo3XkyBFlZWUpNzdX4eHhkqTVq1crIiJCR48eVUhIyO09cAAAAAAAANSLK/4AAABMwG63S5I6d+4sSTp+/LiKi4sVFRXliPH29tawYcO0Z88eSVJeXp4uX77sFBMUFKTQ0FBHTE5OjqxWq6PpJ0mDBw+W1Wp1xFyroqJCZWVlTi8AAAAAAAA0Pxp/AAAALZxhGEpOTta///u/KzQ0VJJUXFwsSQoMDHSKDQwMdKwrLi6Wl5eXOnXq1GBMQEBArTEDAgIcMddKS0tzPA/QarUqODj45g4QAAAAAAAAjULjDwAAoIWbPn26Dhw4oI0bN9ZaZ7FYnN4bhlFr2bWujakrvqH9zJ8/X3a73fE6efJkYw4DAAAAAAAAN4nGHwAAQAs2Y8YMvf/++/r000911113OZbbbDZJqnVVXklJieMqQJvNpsrKSpWWljYYc+bMmVrjnj17ttbVhDW8vb3VsWNHpxcAAAAAAACaH40/AACAFsgwDE2fPl3vvvuuPvnkE/Xq1ctpfa9evWSz2ZSdne1YVllZqZ07d2rIkCGSpLCwMHl6ejrFFBUV6dChQ46YiIgI2e127du3zxGzd+9e2e12RwwAAAAAAADcg4erJwAAAICmmzZtmjZs2KD33ntPfn5+jiv7rFarfH19ZbFYlJSUpNTUVPXu3Vu9e/dWamqq2rVrp7i4OEfshAkT9MILL6hLly7q3LmzZs+erQEDBigyMlKS1K9fP40aNUoTJ07UqlWrJEmTJk1STEyMQkJCXHPwAAAAAAAAqBONPwAAgBbo9ddflyQNHz7cafkbb7yhxMRESdKcOXN06dIlTZ06VaWlpQoPD9fWrVvl5+fniF+2bJk8PDwUGxurS5cuacSIEcrMzFTbtm0dMevXr9fMmTMVFRUlSRozZoxWrFjRvAcIAAAAAACAJrMYhmG4ehK3U1lZmaxWq+x2e7M8b8ZiueW7BEzLLNmH8x5ovOY475v7ZztuXrPXXy+TiIHGMBaZpPgS5z3QWM113lN/ub9m/zfiF2Ggcczyxy+J8x5orGY675vys51n/AEAAAAAAAAAAAAmQOMPAAAAAAAAAAAAMAEafwAAAAAAAAAAAIAJ0PgDAAAAAAAAAAAATIDGHwAAAAAAAAAAAGACNP4AAAAAAAAAAAAAE6DxBwAAAAAAAAAAAJgAjT8AAAAAAAAAAADABGj8AQAAAAAAAAAAACZA4w8AAAAAAAAAAAAwARp/AAAAAAAAAAAAgAnQ+AMAAAAAAAAAAABMgMYfAAAAAAAAAAAAYAI0/gAAAAAAAAAAAAAToPEHAAAAAAAAAAAAmACNPwAAAAAAAAAAAMAEaPwBAAAAAAAAAAAAJkDjDwAAAAAAAK1GaWmp4uPjZbVaZbVaFR8frwsXLjS4jWEYSklJUVBQkHx9fTV8+HAdPnzYKaaiokIzZsyQv7+/2rdvrzFjxujUqVM3NHZmZqbuvfde+fj4yGazafr06Td72AAAoJWg8QcAAAAAAIBWIy4uTgUFBcrKylJWVpYKCgoUHx/f4DZLlizR0qVLtWLFCu3fv182m00jR45UeXm5IyYpKUlbtmzRpk2btHv3bl28eFExMTGqqqpq0thLly7VggULNG/ePB0+fFjbt29XdHT0rf0QAACAaXm4egIAAAAAAADA7XDkyBFlZWUpNzdX4eHhkqTVq1crIiJCR48eVUhISK1tDMNQenq6FixYoHHjxkmS1q5dq8DAQG3YsEGTJ0+W3W5XRkaG3nzzTUVGRkqS3nrrLQUHB2vbtm2Kjo5u1NilpaV68cUX9cEHH2jEiBGOOfzgBz9o8LgqKipUUVHheF9WVnZzHxQAAGixuOIPAAAAAAAArUJOTo6sVquj8SZJgwcPltVq1Z49e+rc5vjx4youLlZUVJRjmbe3t4YNG+bYJi8vT5cvX3aKCQoKUmhoqCOmMWNnZ2erurpap0+fVr9+/XTXXXcpNjZWJ0+ebPC40tLSHLcPtVqtCg4ObuInAwAAzILGHwAAAAAAAFqF4uJiBQQE1FoeEBCg4uLiereRpMDAQKflgYGBjnXFxcXy8vJSp06dGoy53tj//Oc/VV1drdTUVKWnp+vPf/6zzp8/r5EjR6qysrLe45o/f77sdrvjdb1GIQAAMC8afwAAAAAAAGjRUlJSZLFYGnx9/vnnkiSLxVJre8Mw6lz+fdeub8w218Zcb+zq6mpdvnxZy5cvV3R0tAYPHqyNGzfq2LFj+vTTT+sdx9vbWx07dnR6AQCA1oln/AEAAAAAAKBFmz59up588skGY3r27KkDBw7ozJkztdadPXu21hV9NWw2m6SrV+x169bNsbykpMSxjc1mU2VlpUpLS52u+ispKdGQIUMcMdcbu2b//fv3d6zv2rWr/P39VVhY2ODxAQAASFzxBwAAAAAAgBbO399fffv2bfDl4+OjiIgI2e127du3z7Ht3r17ZbfbHQ26a/Xq1Us2m03Z2dmOZZWVldq5c6djm7CwMHl6ejrFFBUV6dChQ46Yxoz90EMPSZKOHj3qiDl//rzOnTunHj163OzHBAAAWgEafwAAAAAAAGgV+vXrp1GjRmnixInKzc1Vbm6uJk6cqJiYGIWEhDji+vbtqy1btki6envOpKQkpaamasuWLTp06JASExPVrl07xcXFSZKsVqsmTJigF154Qdu3b1d+fr6efvppDRgwQJGRkY0eu0+fPho7dqxmzZqlPXv26NChQxo/frz69u2rhx9++DZ/WgAAoCXiVp8AAAAAAABoNdavX6+ZM2cqKipKkjRmzBitWLHCKebo0aOy2+2O93PmzNGlS5c0depUlZaWKjw8XFu3bpWfn58jZtmyZfLw8FBsbKwuXbqkESNGKDMzU23btm3S2OvWrdPzzz+vRx99VG3atNGwYcOUlZUlT0/PW/5ZAAAA87EYhmG4ehK3U1lZmaxWq+x2e7M86Pg6z3QG8D1myT6c90DjNcd539w/23Hzmr3+eplEDDSGscgkxZc474HGaq7znvrL/TX7vxG/CAONY5Y/fkmc90BjNdN535Sf7dzqEwAAAAAAAAAAADABlzf+Vq5cqV69esnHx0dhYWHatWtXvbHvvvuuRo4cqa5du6pjx46KiIjQxx9/fBtnCwAAAAAAAAAAALgnlzb+Nm/erKSkJC1YsED5+fkaOnSoHnnkERUWFtYZ/5e//EUjR47URx99pLy8PD388MN67LHHlJ+ff5tnDgAAAAAAAAAAALgXlzb+li5dqgkTJujZZ59Vv379lJ6eruDgYL3++ut1xqenp2vOnDl64IEH1Lt3b6Wmpqp379764IMPbvPMAQAAAAAAAAAAAPfissZfZWWl8vLyFBUV5bQ8KipKe/bsadQ+qqurVV5ers6dO9cbU1FRobKyMqcXAAAAAAAAAAAAYDYua/ydO3dOVVVVCgwMdFoeGBio4uLiRu3jtdde0zfffKPY2Nh6Y9LS0mS1Wh2v4ODgm5o3AAAAAAAAAAAA4I5ceqtPSbJYLE7vDcOotawuGzduVEpKijZv3qyAgIB64+bPny+73e54nTx58qbnDAAAAAAAAAAAALgbD1cN7O/vr7Zt29a6uq+kpKTWVYDX2rx5syZMmKD//u//VmRkZIOx3t7e8vb2vun5AgAAAAAAAAAAAO7MZVf8eXl5KSwsTNnZ2U7Ls7OzNWTIkHq327hxoxITE7VhwwY9+uijzT1NAAAAAAAAAAAAoEVw2RV/kpScnKz4+HgNGjRIERER+uMf/6jCwkJNmTJF0tXbdJ4+fVrr1q2TdLXpl5CQoP/8z//U4MGDHVcL+vr6ymq1uuw4AAAAAAAAAAAAAFdzaePviSee0L/+9S+98sorKioqUmhoqD766CP16NFDklRUVKTCwkJH/KpVq3TlyhVNmzZN06ZNcywfP368MjMzb/f0AQAAAAAAAAAAALfh0safJE2dOlVTp06tc921zbwdO3Y0/4QAAAAAAAAAAACAFshlz/gDAAAAAAAAAAAAcOvQ+AMAAAAAAAAAAABMgMYfAAAAAAAAAAAAYAI0/gAAAAAAAAAAAAAToPEHAAAAAAAAAAAAmACNPwAAAAAAAAAAAMAEaPwBAAAAAAAAAAAAJkDjDwAAAAAAAAAAADABGn8AAAAAAAAAAACACdD4AwAAAAAAAAAAAEyAxh8AAAAAAAAAAABgAjT+AAAAAAAAAAAAABOg8QcAAAAAAAAAAACYAI0/AAAAAAAAAAAAwARo/AEAAAAAAAAAAAAmQOMPAAAAAAAAAAAAMAEafwAAAAAAAAAAAIAJ0PgDAAAAAAAAAAAATIDGHwAAAAAAAAAAAGACNP4AAAAAAAAAAAAAE6DxBwAAAAAAAAAAAJgAjT8AAAAAAAAAAADABGj8AQAAAAAAAAAAACZA4w8AAAAAAAAAAAAwARp/AAAAAAAAAAAAgAnQ+AMAAAAAAAAAAABMgMYfAAAAAAAAAAAAYAI0/gAAAAAAAAAAAAAToPEHAAAAAAAAAAAAmICHqycAAAAAAAAAoAUxDFfPAAAA1IMr/gAAAAAAAAAAAAAToPEHAAAAAAAAAAAAmAC3+gQAAAAAAAAAAPXjFr9Ai8EVfwAAAAAAAAAAAIAJ0PgDAAAAAAAAAAAATIDGHwAAAAAAAAAAAGACNP4AAAAAAAAAAAAAE6DxBwAAAAAAAAAAAJgAjT8AAAAAAAAAAADABGj8AQAAAAAAAAAAACZA4w8AAAAAAAAAAAAwARp/AAAAAAAAAAAAgAnQ+AMAAAAAAAAAAABMgMYfAAAAAAAAAAAAYAI0/gAAAAAAAAAAAAAToPEHAAAAAAAAAAAAmACNPwAAAAAAAAAAAMAEaPwBAAAAAAAAAAAAJkDjDwAAAAAAAAAAADABGn8AAAAAAAAAAACACdD4AwAAAAAAAAAAAEyAxh8AAAAAAABajdLSUsXHx8tqtcpqtSo+Pl4XLlxocBvDMJSSkqKgoCD5+vpq+PDhOnz4sFNMRUWFZsyYIX9/f7Vv315jxozRqVOnmjR2ZmamLBZLna+SkpJb9REAAAATo/EHAAAAAACAViMuLk4FBQXKyspSVlaWCgoKFB8f3+A2S5Ys0dKlS7VixQrt379fNptNI0eOVHl5uSMmKSlJW7Zs0aZNm7R7925dvHhRMTExqqqqavTYTzzxhIqKipxe0dHRGjZsmAICAm79hwEAAEzHw9UTAAAAAAAAAG6HI0eOKCsrS7m5uQoPD5ckrV69WhERETp69KhCQkJqbWMYhtLT07VgwQKNGzdOkrR27VoFBgZqw4YNmjx5sux2uzIyMvTmm28qMjJSkvTWW28pODhY27ZtU3R0dKPG9vX1la+vr2Pss2fP6pNPPlFGRkZzfzQAAMAkXH7F38qVK9WrVy/5+PgoLCxMu3btajB+586dCgsLk4+Pj/7t3/5Nf/jDH27TTAEAAFqvptZsAAAA7ignJ0dWq9XReJOkwYMHy2q1as+ePXVuc/z4cRUXFysqKsqxzNvbW8OGDXNsk5eXp8uXLzvFBAUFKTQ01BFzI2OvW7dO7dq1009/+tMGj6uiokJlZWVOLwAA0Dq5tPG3efNmJSUlacGCBcrPz9fQoUP1yCOPqLCwsM7448ePa/To0Ro6dKjy8/P1q1/9SjNnztQ777xzm2cOAADQejS1ZgMAAHBXxcXFdd4yMyAgQMXFxfVuI0mBgYFOywMDAx3riouL5eXlpU6dOjUY09Sx16xZo7i4OKerAOuSlpbmeG6g1WpVcHBwg/EAAMC8XHqrz6VLl2rChAl69tlnJUnp6en6+OOP9frrrystLa1W/B/+8Ad1795d6enpkqR+/frp888/1+9//3s9/vjjdY5RUVGhiooKx3u73S5JfPMJcAOchkDr0xznfc3PdMMwbv3OIanpNdttr7++a57dAmZjqt+BOO+BRmmu894d66+UlBS9/PLLDcbs379fkmSxWGqtMwyjzuXfd+36xmxzbUxTxs7JydEXX3yhdevWNTiGJM2fP1/JycmO93a7Xd27dzdX7gcAoBVrSv3lssZfZWWl8vLyNG/ePKflUVFR9d7eICcnx+mWCZIUHR2tjIwMXb58WZ6enrW2SUtLq7Pw45tPgOtZra6eAYDbrTnP+/LycllJLLfcjdRs1F+Ae7L+lhwJtDbNfd67U/01ffp0Pfnkkw3G9OzZUwcOHNCZM2dqrTt79mytK/pq2Gw2SVev2OvWrZtjeUlJiWMbm82myspKlZaWOl31V1JSoiFDhjhimjL2n/70J/3whz9UWFhYg8clXb31qLe3t+N9zR8Hqb8AADCXxtRfLmv8nTt3TlVVVQ3eJuFaxcXFdcZfuXJF586dcyq+alz7jafq6mqdP39eXbp0ue63smAOZWVlCg4O1smTJ9WxY0dXTwfAbcB537oYhqHy8nIFBQW5eiqmdCM1G/UXyMNA68N537q4Y/3l7+8vf3//68ZFRETIbrdr3759evDBByVJe/fuld1udzTortWrVy/ZbDZlZ2dr4MCBkq5+OWrnzp363e9+J0kKCwuTp6ensrOzFRsbK0kqKirSoUOHtGTJkiaPffHiRb399tt13l2hMYKCgnTy5En5+flRf7UC5GCg9eG8b32aUn+59FafUtNvk1BXfF3La1z7jSdJuuOOO25gpmjpOnbsSBIEWhnO+9bDXb5pbmZNqdmov1CDPAy0Ppz3rUdLrb/69eunUaNGaeLEiVq1apUkadKkSYqJiVFISIgjrm/fvkpLS9NPfvITWSwWJSUlKTU1Vb1791bv3r2Vmpqqdu3aKS4uTtLVz2PChAl64YUX1KVLF3Xu3FmzZ8/WgAEDFBkZ2aSxpavPWL5y5YqeeuqpGzrONm3a6K677rqhbdFykYOB1ofzvnVpbP3lssafv7+/2rZtW+ub4t+/TcK1bDZbnfEeHh7q0qVLs80VAACgtbqRmg0AAMCdrV+/XjNnznQ8TmbMmDFasWKFU8zRo0cdzymWpDlz5ujSpUuaOnWqSktLFR4erq1bt8rPz88Rs2zZMnl4eCg2NlaXLl3SiBEjlJmZqbZt2zZpbEnKyMjQuHHjnG4bCgAA0BgWw4VPYg4PD1dYWJhWrlzpWNa/f3+NHTu2zlsZzJ07Vx988IG++OILx7LnnntOBQUFysnJuS1zRstTVlYmq9Uqu93Otx+AVoLzHri1mlqzAeRhoPXhvAcA1yEHA60P5z0a0saVgycnJ+tPf/qT1qxZoyNHjuj5559XYWGhpkyZIunq82ESEhIc8VOmTNHXX3+t5ORkHTlyRGvWrFFGRoZmz57tqkNAC+Dt7a1FixbVuuUYAPPivAdurevVbMC1yMNA68N5DwCuQw4GWh/OezTEpVf8SdLKlSu1ZMkSFRUVKTQ0VMuWLdOPfvQjSVJiYqJOnDihHTt2OOJ37typ559/XocPH1ZQUJDmzp3LH50AAACaWUM1GwAAAAAAANyDyxt/AAAAAAAAAAAAAG6eS2/1CQAAAAAAAAAAAODWoPEHAAAAAAAAAAAAmACNPwAAAAAAAAAAAMAEaPzB5Y4ePSqbzaby8nJXT6XF+9///V8NHDhQ1dXVrp4K4FLukldKSkrUtWtXnT592qXzAIBruUueNAPqL+Aqd8kr1F8A3JW75EkzoP4CrnKXvEL95X5o/LUyiYmJslgsslgs8vT0VGBgoEaOHKk1a9bU+mHZs2dPWSwW5ebmOi1PSkrS8OHDHe9TUlJksVg0ZcoUp7iCggJZLBadOHGiwTktWLBA06ZNk5+fnyRpx44dslgs6tSpk7777jun2H379jnm3xKcOHFCFotFBQUFt2W8mJgYWSwWbdiw4baMB0gtI69I0qpVq3Tfffepffv2uuOOOzRw4ED97ne/c9qurKxMCxcu1A9+8AP5+vqqS5cueuCBB7RkyRKVlpY64oYPH+44Zm9vb91555167LHH9O677zrtLyAgQPHx8Vq0aFGD8wVgbi0hT1J/3TjqL7hCS8grEvUXANdpCXmS+uvGUX/BFVpCXpGov3AVjb9WaNSoUSoqKtKJEyf0f//3f3r44Yc1a9YsxcTE6MqVK06xPj4+mjt37nX36ePjo4yMDH355ZdNmsupU6f0/vvv65lnnqm1zs/PT1u2bHFatmbNGnXv3r1JY7Q2zzzzjP7rv/7L1dNAK+PueSUjI0PJycmaOXOm/va3v+mzzz7TnDlzdPHiRUfM+fPnNXjwYL3xxhuaPXu29u7dq88++0yLFi1SQUFBrV8oJk6cqKKiIv3jH//QO++8o/79++vJJ5/UpEmTnOKeeeYZrV+/3qlwAtD6uHuerEH9dWOov+AK7p5XqL8AuJq758ka1F83hvoLruDueYX6Cw4GWpXx48cbY8eOrbV8+/bthiRj9erVjmU9evQwZs2aZXh5eRkffvihY/msWbOMYcOGOd4vWrTIuO+++4yRI0caP/vZzxzL8/PzDUnG8ePH653Pa6+9ZgwaNMhp2aeffmpIMl588UUjMjLSsfzbb781rFarsXDhQuPa/3X//Oc/G/379ze8vLyMHj16GL///e+d1vfo0cNYvHixER8fb7Rv397o3r278T//8z9GSUmJMWbMGKN9+/ZGaGiosX//fqftPvvsM2Po0KGGj4+PcddddxkzZswwLl686LTf3/zmN8YzzzxjdOjQwQgODjZWrVrlWC/J6VXzuQ0bNsyYNWuW01hjx441xo8ff9NzPnHihCHJ+Oqrr+r93IFbqSXklbFjxxqJiYkNHsfkyZON9u3bG6dOnapzfXV1teO/6zqHDcMw1qxZY0gysrOznZb37NnTyMjIaHB8AObVEvIk9dfNzZn6C7dbS8gr1F8AXKkl5Enqr5ubM/UXbreWkFeov1CDK/4gSfrxj3+s++67r9Zluj179tSUKVM0f/786943+7e//a3eeecd7d+/v9Hj/uUvf9GgQYPqXBcfH69du3apsLBQkvTOO++oZ8+euv/++53i8vLyFBsbqyeffFIHDx5USkqKFi5cqMzMTKe4ZcuW6aGHHlJ+fr4effRRxcfHKyEhQU8//bT++te/6p577lFCQoIMw5AkHTx4UNHR0Ro3bpwOHDigzZs3a/fu3Zo+fbrTfl977TUNGjRI+fn5mjp1qp577jn9/e9/l3T11gyStG3bNhUVFdX6fK+nqXOWpB49eiggIEC7du1q0ljAreZOecVmsyk3N1dff/11ndtUV1dr8+bNevrpp3XnnXfWGdOYW6yMHz9enTp1qnXMDz74IOckgFrcKU/WoP6i/kLL5k55hfoLgDtypzxZg/qL+gstmzvlFeov1KDxB4e+ffvWed/gF198UcePH9f69esb3P7+++9XbGys5s2b1+gxT5w4oaCgoDrXBQQE6JFHHnEUMGvWrNEvfvGLWnFLly7ViBEjtHDhQvXp00eJiYmaPn26Xn31Vae40aNHa/Lkyerdu7deeukllZeX64EHHtDPfvYz9enTR3PnztWRI0d05swZSdKrr76quLg4JSUlqXfv3hoyZIiWL1+udevWOd17ffTo0Zo6daruuecezZ07V/7+/tqxY4ckqWvXrpKkLl26yGazqXPnzo3+bG5kzjXuvPPO694DGrgd3CWvLFq0SHfccYd69uypkJAQJSYm6u2333YUXmfPntWFCxcUEhLitF1YWJg6dOigDh066Oc///l1x27Tpo369OlT65g5JwHUx13yZA3qL+ovtHzukleovwC4K3fJkzWov6i/0PK5S16h/kINGn9wMAyjzo5+165dNXv2bL300kuqrKxscB+//vWvtWvXLm3durVRY166dEk+Pj71rv/FL36hzMxM/fOf/1ROTo6eeuqpWjFHjhzRQw895LTsoYce0rFjx1RVVeVYdu+99zr+OzAwUJI0YMCAWstKSkokXf0mVWZmpiPpdejQQdHR0aqurtbx48fr3K/FYpHNZnPs42Y1dc41fH199e23396SOQA3w13ySrdu3ZSTk6ODBw9q5syZunz5ssaPH69Ro0Y5fevq2rlu2bJFBQUFio6O1qVLlxo1fl3HzDkJoD7ukie/j/qL+gstm7vkFeovAO7KXfLk91F/UX+hZXOXvEL9hRo0/uBw5MgR9erVq851ycnJunTpklauXNngPu6++25NnDhR8+bNc7r8vj7+/v4NPvBz9OjR+u677zRhwgQ99thj6tKlS62YupJMXWN7eno6/rsmvq5lNUmwurpakydPVkFBgeP1t7/9TceOHdPdd99d535r9nO9y7fbtGlTa46XL1++6TnXOH/+vOPbVoAruVteCQ0N1bRp07R+/XplZ2crOztbO3fuVNeuXXXHHXc4blNSo3v37rrnnnvk5+d33XElqaqqSseOHat1zJyTAOrjbnlSov6i/kJL5255hfoLgLtxtzwpUX9Rf6Glc7e8Qv0FGn+QJH3yySc6ePCgHn/88TrXd+jQQQsXLtRvfvMblZWVNbivl156SV9++aU2bdp03XEHDhyoL774ot71bdu2VXx8vHbs2FHnbQ4kqX///tq9e7fTsj179qhPnz5q27btdedQn/vvv1+HDx/WPffcU+vl5eXVqH3UxH3/m1fS1W97FBUVOd5XVVXp0KFDNzzX7/vuu+/01VdfaeDAgbdkf8CNcte8UqN///6SpG+++UZt2rRRbGys3nrrLZ0+ffq629Zn7dq1Ki0trXXMhw4d4pwEUIu75knqr6aj/oK7cNe8UoP6C4CruWuepP5qOuovuAt3zSs1qL9aJxp/rVBFRYWKi4t1+vRp/fWvf1VqaqrGjh2rmJgYJSQk1LvdpEmTZLVatXHjxgb3HxgYqOTkZC1fvvy6c4mOjlZOTk6twuD7Fi9erLNnzyo6OrrO9S+88IK2b9+uxYsX68svv9TatWu1YsUKzZ49+7rjN2Tu3LnKycnRtGnTVFBQoGPHjun999/XjBkzGr2PgIAA+fr6KisrS2fOnJHdbpd09aGvH374oT788EP9/e9/19SpU3XhwoWbmm+N3NxceXt7KyIi4pbsD2gMd88rzz33nBYvXqzPPvtMX3/9tXJzc5WQkKCuXbs6zpXU1FTdeeedCg8P15o1a3TgwAF99dVX2rJli3Jycmr9IvXtt9+quLhYp06d0t69ezV37lxNmTJFzz33nB5++GGnuLy8PEVFRV137gDMy93z5LWov5qG+guu4O55hfoLgKu5e568FvVX01B/wRXcPa9Qf6EGjb9WKCsrS926dVPPnj01atQoffrpp1q+fLnee++9Br8h5OnpqcWLFzs92Lc+v/zlL9WhQ4frxo0ePVqenp7atm1bvTFeXl7y9/ev8z7J0tVvJr399tvatGmTQkND9dJLL+mVV15RYmLidcdvyL333qudO3fq2LFjGjp0qAYOHKiFCxeqW7dujd6Hh4eHli9frlWrVikoKEhjx46VdPXe7ePHj1dCQoKGDRumXr16OSXKm7Fx40Y99dRTateu3S3ZH9AY7p5XIiMjlZub63gw+OOPPy4fHx9t377dcQuVLl26aN++fUpISNCrr76qBx98UAMGDFBKSoqeeOIJrV692mmc1atXq1u3brr77rv1k5/8RF988YU2b95c69YN7733nrp3766hQ4ded+4AzMvd8+S1qL+ahvoLruDueYX6C4CruXuevBb1V9NQf8EV3D2vUH+hhsVozA1jgWa0cuVKvffee/r4449dPZUW7+zZs+rbt68+//zzeu8rDbQG7pRXHnzwQSUlJSkuLs7VUwEAB3fKky0d9RdwlTvlFeovAO7InfJkS0f9BVzlTnmF+su9eLh6AsCkSZNUWlqq8vLyRj9AFHU7fvy4Vq5cSdGDVs9d8kpJSYl++tOf6uc//7nL5gAAdXGXPGkG1F/AVe6SV6i/ALgrd8mTZkD9BVzlLnmF+sv9cMUfAAAAAAAAAAAAYAI84w8AAAAAAAAAAAAwARp/AAAAAAAAAAAAgAnQ+AMAAAAAAAAAAABMgMYfAAAAAAAAAAAAYAI0/gAAAAAAAAAAAAAToPEHAAAAAAAAAAAAmACNPwAAAAAAAAAAAMAEaPwBAAAAAAAAAAAAJkDjDwAAAAAAAAAAADCB/wdXwsgmz8Gc3gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1800x600 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a DataFrame to store evaluation results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'MSE': mse_values,\n",
    "    'MAE': mae_values,\n",
    "    'R2': r2_values\n",
    "})\n",
    "\n",
    "# Plot the results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# MSE Plot\n",
    "axes[0].bar(results_df['Model'], results_df['MSE'], color='blue')\n",
    "axes[0].set_title('Mean Squared Error (MSE)')\n",
    "axes[0].set_ylabel('MSE')\n",
    "\n",
    "# MAE Plot\n",
    "axes[1].bar(results_df['Model'], results_df['MAE'], color='green')\n",
    "axes[1].set_title('Mean Absolute Error (MAE)')\n",
    "axes[1].set_ylabel('MAE')\n",
    "\n",
    "# R2 Plot\n",
    "axes[2].bar(results_df['Model'], results_df['R2'], color='red')\n",
    "axes[2].set_title('R2 Score')\n",
    "axes[2].set_ylabel('R2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2712e19f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dnn_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Save the best model based on evaluation criteria (e.g., DNN with Adam optimizer)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m best_model \u001b[38;5;241m=\u001b[39m dnn_model  \u001b[38;5;66;03m# Assuming DNN (Adam) has the best performance\u001b[39;00m\n\u001b[0;32m      3\u001b[0m best_model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_insurance_predictor_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dnn_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the best model based on evaluation criteria (e.g., DNN with Adam optimizer)\n",
    "best_model = dnn_model  # Assuming DNN (Adam) has the best performance\n",
    "best_model.save('best_insurance_predictor_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ef41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
